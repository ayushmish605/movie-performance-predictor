{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a33bbfe4",
   "metadata": {},
   "source": [
    "# Full Dataset: Load & Scrape All Movies\n",
    "\n",
    "This notebook provides a **complete scraping workflow**:\n",
    "- Load ALL movies from TMDB CSV\n",
    "- Scrape IMDB reviews for all movies\n",
    "- Scrape Rotten Tomatoes Tomatometer scores and reviews for all movies\n",
    "- Display movie metadata (title, genres, overview)\n",
    "- Show TMDB ratings from CSV\n",
    "- Demonstrate smart rating selection logic\n",
    "\n",
    "**Runtime:** Variable depending on number of movies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1534da76",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1195536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful!\n",
      "üìÅ Project root: /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys\n",
      "‚úÖ OpenAI API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add src to path\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root / 'src'))\n",
    "\n",
    "# Load environment variables ONCE at the top\n",
    "load_dotenv(project_root / '.env', override=True)\n",
    "\n",
    "import pandas as pd\n",
    "from database.db import SessionLocal, init_db\n",
    "from database.models import Movie\n",
    "from data_ingestion.tmdb_loader import TMDBDataLoader\n",
    "\n",
    "print(\" Imports successful!\")\n",
    "print(f\" Project root: {project_root}\")\n",
    "\n",
    "# Check API keys (DO NOT PRINT THE ACTUAL KEY!)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\" OpenAI API key loaded successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Warning: OPENAI_API_KEY not found in .env file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7473fa1",
   "metadata": {},
   "source": [
    "## Step 2: Check CSV File\n",
    "\n",
    "Make sure your TMDB CSV is in the `data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abbfc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CSV exists\n",
    "try:\n",
    "    csv_path = project_root / \"data\" / \"tmdb_commercial_movies_2016_2024.csv\"\n",
    "    \n",
    "    if csv_path.exists():\n",
    "        print(f\" CSV found: {csv_path.name}\")\n",
    "        print(f\"   Size: {csv_path.stat().st_size / (1024*1024):.1f} MB\")\n",
    "    else:\n",
    "        print(f\" CSV not found!\")\n",
    "        print(f\"\\nPlease copy your CSV to:\")\n",
    "        print(f\"   {csv_path}\")\n",
    "        print(f\"\\nCommand:\")\n",
    "        print(f\"   cp ~/Downloads/tmdb_commercial_movies_2016_2024.csv {project_root}/data/\")\n",
    "except Exception as e:\n",
    "    print(f\" Error checking CSV: {e}\")\n",
    "    print(f\"   Make sure you ran Step 1 first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92023694",
   "metadata": {},
   "source": [
    "## Step 3: Load CSV Preview\n",
    "\n",
    "Let's peek at the CSV structure without loading into database yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV with pandas to inspect\n",
    "if csv_path.exists():\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    print(f\" CSV Statistics:\")\n",
    "    print(f\"   Total rows: {len(df):,}\")\n",
    "    print(f\"   Columns: {len(df.columns)}\")\n",
    "    print(f\"\\n Column names:\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        print(f\"   {i}. {col}\")\n",
    "    \n",
    "    print(f\"\\n First 3 movies:\")\n",
    "    display(df.head(3)[['title', 'release_date', 'vote_average', 'vote_count']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bdfd41",
   "metadata": {},
   "source": [
    "## Step 4: Initialize Database & Clear Old Data\n",
    "\n",
    "Create the database schema and clear any existing data for a fresh start.\n",
    "\n",
    "‚ö†Ô∏è **IMPORTANT:** Run this step **ONCE** before scraping to avoid duplicates!\n",
    "- If you re-run scraping steps (11a, 11b) multiple times, they will now skip duplicates automatically\n",
    "- But for a completely fresh start, run this step to clear everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules to get latest code changes\n",
    "import importlib\n",
    "import sys\n",
    "\n",
    "# Remove cached modules\n",
    "for module in list(sys.modules.keys()):\n",
    "    if module.startswith('database') or module.startswith('data_ingestion'):\n",
    "        del sys.modules[module]\n",
    "\n",
    "# Re-import\n",
    "from database.db import SessionLocal, init_db, DEFAULT_DB_PATH\n",
    "from database.models import Movie, Review, MovieSearchTerm\n",
    "\n",
    "# Check if database exists and has old schema\n",
    "db_path = project_root / \"data\" / \"database\" / \"movie_recommender.db\"\n",
    "\n",
    "if db_path.exists():\n",
    "    print(f\"  Found existing database at: {db_path}\")\n",
    "    print(\"   This database might not have the new sentiment analysis columns.\")\n",
    "    print(\"   Deleting old database to create fresh one with new schema...\")\n",
    "    db_path.unlink()\n",
    "    print(\"    Old database deleted!\")\n",
    "\n",
    "# Initialize database with new schema\n",
    "print(\"\\nüóÑÔ∏è  Initializing database with new schema...\")\n",
    "init_db()\n",
    "\n",
    "# CLEAR ALL EXISTING DATA for fresh start (should be empty since we deleted the file)\n",
    "print(\" Clearing existing database data (if any)...\")\n",
    "db = SessionLocal()\n",
    "try:\n",
    "    # Count existing records\n",
    "    movie_count = db.query(Movie).count()\n",
    "    review_count = db.query(Review).count()\n",
    "    search_term_count = db.query(MovieSearchTerm).count()\n",
    "    \n",
    "    if movie_count > 0 or review_count > 0 or search_term_count > 0:\n",
    "        # Delete all records (cascades to related tables due to relationships)\n",
    "        db.query(Review).delete()\n",
    "        db.query(MovieSearchTerm).delete()\n",
    "        db.query(Movie).delete()\n",
    "        db.commit()\n",
    "        \n",
    "        print(f\"    Deleted {movie_count} movies\")\n",
    "        print(f\"    Deleted {review_count} reviews\")\n",
    "        print(f\"    Deleted {search_term_count} search terms\")\n",
    "    \n",
    "    print(\"    Database cleared - starting fresh!\")\n",
    "except Exception as e:\n",
    "    print(f\"     Error clearing database: {e}\")\n",
    "    db.rollback()\n",
    "finally:\n",
    "    db.close()\n",
    "\n",
    "print(\"\\n Database initialized with new schema (including sentiment columns)!\")\n",
    "print(f\" Current database: 0 movies\")\n",
    "print(\"\\n New columns added:\")\n",
    "print(\"   Movies table: rt_tomatometer, rt_tomatometer_out_of_10,\")\n",
    "print(\"                 sentiment_imdb_avg, sentiment_rt_top_critics_avg,\")\n",
    "print(\"                 sentiment_rt_all_critics_avg, sentiment_rt_verified_audience_avg,\")\n",
    "print(\"                 sentiment_rt_all_audience_avg\")\n",
    "print(\"   Reviews table: review_category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f3ffd2",
   "metadata": {},
   "source": [
    "Print the database schema neatly so we can analyze it visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8992cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoInspectionAvailable",
     "evalue": "No inspection system is available for object of type <class 'sqlite3.Connection'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNoInspectionAvailable\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msqlalchemy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m inspect\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m inspector = \u001b[43minspect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# or use your SQLAlchemy engine\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m table_name \u001b[38;5;129;01min\u001b[39;00m inspector.get_table_names():\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys/venv/lib/python3.11/site-packages/sqlalchemy/inspection.py:147\u001b[39m, in \u001b[36minspect\u001b[39m\u001b[34m(subject, raiseerr)\u001b[39m\n\u001b[32m    144\u001b[39m     reg = ret = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m raiseerr \u001b[38;5;129;01mand\u001b[39;00m (reg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc.NoInspectionAvailable(\n\u001b[32m    148\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo inspection system is \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    149\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mavailable for object of type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % type_\n\u001b[32m    150\u001b[39m     )\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[31mNoInspectionAvailable\u001b[39m: No inspection system is available for object of type <class 'sqlite3.Connection'>"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import inspect\n",
    "\n",
    "inspector = inspect(conn)  # or use your SQLAlchemy engine\n",
    "for table_name in inspector.get_table_names():\n",
    "    print(f\"Table: {table_name}\")\n",
    "    for column in inspector.get_columns(table_name):\n",
    "        print(f\"  {column['name']} ({column['type']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c75b900",
   "metadata": {},
   "source": [
    "## Step 5: Load All Movies into Database\n",
    "\n",
    "We'll load **ALL** movies from the CSV file into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c713f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: movies\n",
      "  id (INTEGER)\n",
      "  tmdb_id (INTEGER)\n",
      "  imdb_id (VARCHAR(20))\n",
      "  title (VARCHAR(500))\n",
      "  release_year (INTEGER)\n",
      "  genres (VARCHAR(500))\n",
      "  overview (TEXT)\n",
      "  runtime (INTEGER)\n",
      "  language (VARCHAR(10))\n",
      "  tmdb_rating (FLOAT)\n",
      "  tmdb_vote_count (INTEGER)\n",
      "  popularity (FLOAT)\n",
      "  imdb_rating (FLOAT)\n",
      "  imdb_vote_count (INTEGER)\n",
      "  scraped_at (DATETIME)\n",
      "  rt_tomatometer (FLOAT)\n",
      "  rt_tomatometer_out_of_10 (FLOAT)\n",
      "  sentiment_imdb_avg (FLOAT)\n",
      "  sentiment_rt_top_critics_avg (FLOAT)\n",
      "  sentiment_rt_all_critics_avg (FLOAT)\n",
      "  sentiment_rt_verified_audience_avg (FLOAT)\n",
      "  sentiment_rt_all_audience_avg (FLOAT)\n",
      "  created_at (DATETIME)\n",
      "  updated_at (DATETIME)\n",
      "  rt_slug (VARCHAR(200))\n",
      "Table: movie_search_terms\n",
      "  id (INTEGER)\n",
      "  movie_id (INTEGER)\n",
      "  search_term (VARCHAR(200))\n",
      "  term_type (VARCHAR(50))\n",
      "  source (VARCHAR(50))\n",
      "  created_at (DATETIME)\n",
      "Table: reviews\n",
      "  id (INTEGER)\n",
      "  movie_id (INTEGER)\n",
      "  source (VARCHAR(50))\n",
      "  source_id (VARCHAR(200))\n",
      "  review_category (VARCHAR(50))\n",
      "  text (TEXT)\n",
      "  rating (FLOAT)\n",
      "  title (VARCHAR(500))\n",
      "  author (VARCHAR(200))\n",
      "  author_id (VARCHAR(200))\n",
      "  upvotes (INTEGER)\n",
      "  downvotes (INTEGER)\n",
      "  helpful_count (INTEGER)\n",
      "  not_helpful_count (INTEGER)\n",
      "  reply_count (INTEGER)\n",
      "  review_date (DATETIME)\n",
      "  scraped_at (DATETIME)\n",
      "  review_length (INTEGER)\n",
      "  word_count (INTEGER)\n",
      "  quality_score (FLOAT)\n",
      "  sentiment_score (FLOAT)\n",
      "  sentiment_label (VARCHAR(20))\n",
      "  sentiment_confidence (FLOAT)\n",
      "  is_processed (BOOLEAN)\n",
      "  is_valid (BOOLEAN)\n",
      "Table: movie_embeddings\n",
      "  id (INTEGER)\n",
      "  movie_id (INTEGER)\n",
      "  embedding_type (VARCHAR(50))\n",
      "  embedding_vector (JSON)\n",
      "  model_name (VARCHAR(100))\n",
      "  created_at (DATETIME)\n",
      "Table: user_ratings\n",
      "  id (INTEGER)\n",
      "  user_id (INTEGER)\n",
      "  movie_id (INTEGER)\n",
      "  rating (FLOAT)\n",
      "  timestamp (DATETIME)\n",
      "Table: scraping_logs\n",
      "  id (INTEGER)\n",
      "  movie_id (INTEGER)\n",
      "  source (VARCHAR(50))\n",
      "  status (VARCHAR(20))\n",
      "  reviews_collected (INTEGER)\n",
      "  error_message (TEXT)\n",
      "  timestamp (DATETIME)\n",
      "  duration_seconds (FLOAT)\n"
     ]
    }
   ],
   "source": [
    "# Show database schema for all tables (SQLite fallback)\n",
    "import sqlite3\n",
    "\n",
    "# Use db_path if available, else fallback to default\n",
    "try:\n",
    "    conn_sqlite = sqlite3.connect(str(db_path))\n",
    "except Exception:\n",
    "    conn_sqlite = sqlite3.connect('src/database/movie_recommender.db')\n",
    "\n",
    "cursor = conn_sqlite.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "for table in tables:\n",
    "    print(f\"Table: {table[0]}\")\n",
    "    cursor.execute(f\"PRAGMA table_info({table[0]});\")\n",
    "    for col in cursor.fetchall():\n",
    "        print(f\"  {col[1]} ({col[2]})\")\n",
    "conn_sqlite.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979a0a95",
   "metadata": {},
   "source": [
    "## Step 6: View Movies with Ratings\n",
    "\n",
    "Display all loaded movies with their metadata and rating information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e7e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch all movies\n",
    "# db = SessionLocal()\n",
    "# movies = db.query(Movie).all()\n",
    "# db.close()\n",
    "\n",
    "print(f\"Showing {len(movies)} movies:\\n\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "# Display first 10 for preview\n",
    "for i, movie in enumerate(movies[:10], 1):\n",
    "    print(f\"\\n{i}. {movie.title} ({movie.release_year})\")\n",
    "    print(f\"   Genres: {movie.genres or 'N/A'}\")\n",
    "    \n",
    "    if movie.overview:\n",
    "        overview_preview = movie.overview[:150] + \"...\" if len(movie.overview) > 150 else movie.overview\n",
    "        print(f\"   Overview: {overview_preview}\")\n",
    "    \n",
    "    # Get rating metadata\n",
    "    rating_info = movie.get_rating_metadata()\n",
    "    \n",
    "    print(f\"\\n   Recommended Rating: {rating_info['recommended_rating']}/10\")\n",
    "    \n",
    "    # Show all rating sources\n",
    "    if rating_info['sources']:\n",
    "        print(f\"   Rating Sources:\")\n",
    "        for source in rating_info['sources']:\n",
    "            votes = f\"{source['votes']:,}\" if source['votes'] else \"N/A\"\n",
    "            age = f\"{source['age_days']} days ago\" if source['age_days'] is not None else \"unknown age\"\n",
    "            source_name = source['source'].replace('_', ' ').title()\n",
    "            print(f\"      - {source_name}: {source['rating']}/10 ({votes} votes, {age})\")\n",
    "    \n",
    "    # Show difference if both ratings exist\n",
    "    if 'difference' in rating_info and rating_info['difference'] > 0:\n",
    "        print(f\"\\n   {rating_info['note']} (difference: {rating_info['difference']})\")\n",
    "    \n",
    "    print(\"   \" + \"-\" * 96)\n",
    "\n",
    "if len(movies) > 10:\n",
    "    print(f\"\\n... and {len(movies) - 10} more movies in database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc052a",
   "metadata": {},
   "source": [
    "## Step 7: Create DataFrame for Analysis\n",
    "\n",
    "Convert to pandas for easy analysis and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7627a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with all movies\n",
    "# db = SessionLocal()\n",
    "# movies = db.query(Movie).all()\n",
    "# db.close()\n",
    "\n",
    "data = []\n",
    "for movie in movies:\n",
    "    rating_info = movie.get_rating_metadata()\n",
    "    data.append({\n",
    "        'title': movie.title,\n",
    "        'year': movie.release_year,\n",
    "        'genres': movie.genres,\n",
    "        'rating': rating_info['recommended_rating'],\n",
    "        'tmdb_rating': movie.tmdb_rating,\n",
    "        'tmdb_votes': movie.tmdb_vote_count,\n",
    "        'imdb_rating': movie.imdb_rating,\n",
    "        'imdb_votes': movie.imdb_vote_count,\n",
    "        'popularity': movie.popularity\n",
    "    })\n",
    "\n",
    "df_movies = pd.DataFrame(data)\n",
    "print(f\"Movie DataFrame ({len(df_movies)} movies):\")\n",
    "display(df_movies[['title', 'year', 'rating', 'tmdb_rating', 'tmdb_votes']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ff35fa",
   "metadata": {},
   "source": [
    "## Step 8: Basic Visualizations\n",
    "\n",
    "Simple charts to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e90ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Rating Distribution\n",
    "ax1 = axes[0, 0]\n",
    "df_movies['rating'].hist(bins=10, ax=ax1, color='skyblue', edgecolor='black')\n",
    "ax1.set_title('Rating Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Rating (0-10)')\n",
    "ax1.set_ylabel('Number of Movies')\n",
    "ax1.axvline(df_movies['rating'].mean(), color='red', linestyle='--', label=f\"Mean: {df_movies['rating'].mean():.2f}\")\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Top Movies by Rating\n",
    "ax2 = axes[0, 1]\n",
    "top_movies = df_movies.nlargest(5, 'rating')\n",
    "ax2.barh(top_movies['title'], top_movies['rating'], color='coral')\n",
    "ax2.set_title('Top 5 Movies by Rating', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Rating')\n",
    "ax2.invert_yaxis()\n",
    "\n",
    "# 3. Vote Count vs Rating\n",
    "ax3 = axes[1, 0]\n",
    "ax3.scatter(df_movies['tmdb_votes'], df_movies['rating'], s=100, alpha=0.6, color='green')\n",
    "ax3.set_title('Vote Count vs Rating', fontsize=14, fontweight='bold')\n",
    "ax3.set_xlabel('TMDB Vote Count')\n",
    "ax3.set_ylabel('Rating')\n",
    "\n",
    "# 4. Movies by Year\n",
    "ax4 = axes[1, 1]\n",
    "year_counts = df_movies['year'].value_counts().sort_index()\n",
    "ax4.bar(year_counts.index, year_counts.values, color='purple', alpha=0.7)\n",
    "ax4.set_title('Movies by Release Year', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Year')\n",
    "ax4.set_ylabel('Number of Movies')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Visualizations complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54529313",
   "metadata": {},
   "source": [
    "**What we accomplished:**\n",
    "-  Loaded ALL movies from CSV\n",
    "-  Scraped IMDB ratings for all movies\n",
    "-  Scraped IMDB reviews for all movies\n",
    "-  Scraped Rotten Tomatoes Tomatometer scores and reviews\n",
    "-  Smart rating selection logic working\n",
    "-  Database operations successful\n",
    "\n",
    "**Next Steps:**\n",
    "1. Analyze the scraped data\n",
    "2. Run sentiment analysis on reviews\n",
    "3. Build recommendation models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aed77c3",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b873d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total movies loaded: {len(df_movies)}\")\n",
    "print(f\"Average rating: {df_movies['rating'].mean():.2f}/10\")\n",
    "print(f\"Rating range: {df_movies['rating'].min():.1f} - {df_movies['rating'].max():.1f}\")\n",
    "print(f\"Total votes (TMDB): {df_movies['tmdb_votes'].sum():,}\")\n",
    "print(f\"Average votes per movie: {df_movies['tmdb_votes'].mean():,.0f}\")\n",
    "\n",
    "# Movies with IMDB ratings\n",
    "movies_with_imdb = df_movies['imdb_rating'].notna().sum()\n",
    "if movies_with_imdb > 0:\n",
    "    print(f\"\\nMovies with IMDB ratings: {movies_with_imdb}/{len(df_movies)}\")\n",
    "    print(f\"Average IMDB rating: {df_movies['imdb_rating'].mean():.2f}/10\")\n",
    "print()\n",
    "\n",
    "# Genre analysis\n",
    "print(\"GENRE BREAKDOWN:\")\n",
    "all_genres = []\n",
    "for genres_str in df_movies['genres'].dropna():\n",
    "    all_genres.extend(genres_str.split('|'))\n",
    "\n",
    "from collections import Counter\n",
    "genre_counts = Counter(all_genres)\n",
    "for genre, count in genre_counts.most_common():\n",
    "    print(f\"   {genre}: {count} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7aac5a9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Test Scraping Pipeline\n",
    "\n",
    "Now let's test the scraping functionality on a few movies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbbc2eb",
   "metadata": {},
   "source": [
    "## Step 9: Import Scraping Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0fa716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All modules reloaded with clean logging!\n",
      "üìä IMDb scraper logger handlers: 1\n",
      "üìä Root logger handlers: 0\n"
     ]
    }
   ],
   "source": [
    "# Reload scraper modules to get latest code changes\n",
    "import importlib\n",
    "import logging\n",
    "\n",
    "# Step 1: Clear ALL loggers and handlers\n",
    "for logger_name in list(logging.Logger.manager.loggerDict.keys()):\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.handlers.clear()\n",
    "    logger.propagate = False\n",
    "\n",
    "# Clear root logger\n",
    "logging.root.handlers.clear()\n",
    "\n",
    "# Step 2: Remove ALL cached modules\n",
    "modules_to_remove = [m for m in sys.modules.keys() if 'scrapers' in m or 'utils.logger' in m]\n",
    "for module in modules_to_remove:\n",
    "    del sys.modules[module]\n",
    "\n",
    "# Step 3: Import scraping modules (this will create fresh loggers)\n",
    "from scrapers.imdb_scraper import IMDbScraper\n",
    "from database.models import Review, MovieSearchTerm\n",
    "from datetime import datetime\n",
    "\n",
    "print(\" All modules reloaded with clean logging!\")\n",
    "print(f\" IMDb scraper logger handlers: {len(logging.getLogger('scrapers.imdb_scraper').handlers)}\")\n",
    "print(f\" Root logger handlers: {len(logging.root.handlers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883636a7",
   "metadata": {},
   "source": [
    "## Step 10: Select Movies for Scraping\n",
    "\n",
    "Select random movies from the database to test the scraping pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0160b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select movies that DON'T have IMDb ratings yet\n",
    "db = SessionLocal()\n",
    "\n",
    "# Get movies without IMDb ratings\n",
    "movies_to_scrape = db.query(Movie).filter(Movie.imdb_rating.is_(None)).all()\n",
    "\n",
    "# Count movies that already have ratings\n",
    "movies_already_scraped = db.query(Movie).filter(Movie.imdb_rating.isnot(None)).count()\n",
    "\n",
    "db.close()\n",
    "\n",
    "if not movies_to_scrape:\n",
    "    print(\"‚ö†Ô∏è  No movies need IMDb rating scraping!\")\n",
    "    if movies_already_scraped > 0:\n",
    "        print(f\"‚ÑπÔ∏è  All {movies_already_scraped} movies already have IMDb ratings\")\n",
    "else:\n",
    "    print(f\" Selected {len(movies_to_scrape)} movies needing IMDb ratings\")\n",
    "    if movies_already_scraped > 0:\n",
    "        print(f\" Skipping {movies_already_scraped} movies that already have IMDb ratings\\n\")\n",
    "    \n",
    "    print(f\"üìΩÔ∏è  First 10 movies to scrape:\")\n",
    "    for i, movie in enumerate(movies_to_scrape[:10], 1):\n",
    "        print(f\"   {i}. {movie.title} ({movie.release_year})\")\n",
    "    \n",
    "    if len(movies_to_scrape) > 10:\n",
    "        print(f\"\\n   ... and {len(movies_to_scrape) - 10} more movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11696d7b",
   "metadata": {},
   "source": [
    "## Step 11: Scrape IMDb Ratings for All Movies (Concurrent)\n",
    "\n",
    "Fetch live ratings from IMDb for **ALL** movies.\n",
    "**Scrapes up to 3 movies concurrently** to respect rate limits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9dcc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize IMDb scraper\n",
    "imdb = IMDbScraper()\n",
    "\n",
    "if not movies_to_scrape:\n",
    "    print(\" No IMDb scraping needed - all movies already have ratings!\")\n",
    "else:\n",
    "    print(f\" Scraping IMDb ratings for {len(movies_to_scrape)} movies concurrently...\\n\")\n",
    "    \n",
    "    # Concurrent IMDb scraping with batch commits\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    \n",
    "    successful_imdb = 0\n",
    "    failed_imdb = 0\n",
    "    \n",
    "    def scrape_imdb_for_movie(movie):\n",
    "        \"\"\"Scrape IMDb rating for a single movie\"\"\"\n",
    "        try:\n",
    "            # Scrape IMDb data\n",
    "            imdb_data = imdb.scrape_movie_rating(\n",
    "                title=movie.title,\n",
    "                year=movie.release_year\n",
    "            )\n",
    "            \n",
    "            if imdb_data and imdb_data.get('rating'):\n",
    "                return (movie.id, movie.title, imdb_data, True)\n",
    "            else:\n",
    "                return (movie.id, movie.title, None, False)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    {movie.title}: Error - {str(e)[:100]}\")\n",
    "            return (movie.id, movie.title, None, False)\n",
    "    \n",
    "    # Process in batches of 10 with concurrent scraping\n",
    "    batch_size = 10\n",
    "    total = len(movies_to_scrape)\n",
    "    \n",
    "    for batch_start in range(0, total, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total)\n",
    "        batch_movies = movies_to_scrape[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\" BATCH {(batch_start // batch_size) + 1}: Scraping movies {batch_start + 1}-{batch_end} of {total}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Run concurrent scraping for this batch (5 concurrent workers)\n",
    "        batch_results = []\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = {executor.submit(scrape_imdb_for_movie, movie): movie for movie in batch_movies}\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                batch_results.append(future.result())\n",
    "        \n",
    "        # Save this batch to database\n",
    "        db = SessionLocal()\n",
    "        batch_successful = 0\n",
    "        batch_failed = 0\n",
    "        \n",
    "        try:\n",
    "            for movie_id, movie_title, imdb_data, success in batch_results:\n",
    "                if success and imdb_data:\n",
    "                    # Get the movie from database using ID\n",
    "                    movie_obj = db.query(Movie).filter(Movie.id == movie_id).first()\n",
    "                    if movie_obj:\n",
    "                        movie_obj.imdb_rating = imdb_data['rating']\n",
    "                        movie_obj.imdb_vote_count = imdb_data.get('vote_count')\n",
    "                        movie_obj.imdb_id = imdb_data.get('imdb_id')\n",
    "                        movie_obj.scraped_at = datetime.now()\n",
    "                        batch_successful += 1\n",
    "                        print(f\"    {movie_title}: {imdb_data['rating']}/10\")\n",
    "                else:\n",
    "                    batch_failed += 1\n",
    "            \n",
    "            # Commit this batch\n",
    "            db.commit()\n",
    "            \n",
    "            # Update totals\n",
    "            successful_imdb += batch_successful\n",
    "            failed_imdb += batch_failed\n",
    "            \n",
    "            print(f\"\\n Batch {(batch_start // batch_size) + 1} committed to database!\")\n",
    "            print(f\"   Successful: {batch_successful}/{len(batch_movies)}\")\n",
    "            if batch_failed > 0:\n",
    "                print(f\"   Failed: {batch_failed}/{len(batch_movies)}\")\n",
    "            print(f\"\\n Running totals: {successful_imdb} successful, {failed_imdb} failed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" Database error in batch {(batch_start // batch_size) + 1}: {e}\")\n",
    "            db.rollback()\n",
    "            print(\"   ‚ö†Ô∏è  Batch rolled back - continuing to next batch...\")\n",
    "        finally:\n",
    "            db.close()\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\" IMDB RATING SCRAPING COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\" IMDb ratings updated: {successful_imdb}/{total}\")\n",
    "    print(f\" Failed: {failed_imdb}/{total}\")\n",
    "    print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cca1f9",
   "metadata": {},
   "source": [
    "## Step 11a: Scrape IMDb Reviews for All Movies (Concurrent)\n",
    "\n",
    "Now let's scrape detailed **user reviews** from IMDb for deeper analysis.\n",
    "**Scrapes up to 3 movies concurrently** with up to 30 reviews per movie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2804e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Scraping IMDb REVIEWS for movies without existing reviews...\\n\")\n",
    "\n",
    "# Get ALL movies that have IMDb IDs but NO existing IMDb reviews\n",
    "db = SessionLocal()\n",
    "\n",
    "# Subquery to find movies that already have IMDb reviews\n",
    "movies_with_reviews_subquery = db.query(Review.movie_id).filter(\n",
    "    Review.source == 'imdb'\n",
    ").distinct().subquery()\n",
    "\n",
    "# Get movies that have IMDb IDs but are NOT in the subquery\n",
    "movies_with_imdb = db.query(Movie).filter(\n",
    "    Movie.imdb_id.isnot(None),\n",
    "    ~Movie.id.in_(db.query(movies_with_reviews_subquery.c.movie_id))\n",
    ").all()\n",
    "\n",
    "# Also count how many already have reviews\n",
    "movies_already_scraped = db.query(Movie).filter(\n",
    "    Movie.imdb_id.isnot(None),\n",
    "    Movie.id.in_(db.query(movies_with_reviews_subquery.c.movie_id))\n",
    ").count()\n",
    "\n",
    "db.close()\n",
    "\n",
    "if not movies_with_imdb:\n",
    "    print(\"‚ö†Ô∏è  No movies need IMDb review scraping!\")\n",
    "    if movies_already_scraped > 0:\n",
    "        print(f\"‚ÑπÔ∏è  {movies_already_scraped} movies already have IMDb reviews\")\n",
    "else:\n",
    "    print(f\" Found {len(movies_with_imdb)} movies needing IMDb reviews\")\n",
    "    if movies_already_scraped > 0:\n",
    "        print(f\" Skipping {movies_already_scraped} movies that already have IMDb reviews\\n\")\n",
    "    \n",
    "    # Statistics\n",
    "    successful_imdb_reviews = 0\n",
    "    total_reviews_added = 0\n",
    "    skipped_duplicates = 0\n",
    "    \n",
    "    def scrape_imdb_reviews_for_movie(movie):\n",
    "        \"\"\"Scrape IMDb reviews for a single movie\"\"\"\n",
    "        try:\n",
    "            # Scrape reviews (max 30 per movie)\n",
    "            reviews_data = imdb.scrape_reviews(\n",
    "                imdb_id=movie.imdb_id,\n",
    "                max_reviews=30\n",
    "            )\n",
    "            \n",
    "            if reviews_data:\n",
    "                return (movie.id, movie.title, movie.imdb_id, reviews_data, True)\n",
    "            else:\n",
    "                return (movie.id, movie.title, movie.imdb_id, [], False)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    {movie.title}: Error - {str(e)[:100]}\")\n",
    "            return (movie.id, movie.title, movie.imdb_id, [], False)\n",
    "    \n",
    "    # Process in batches of 10 with concurrent scraping\n",
    "    batch_size = 10\n",
    "    total_movies = len(movies_with_imdb)\n",
    "    \n",
    "    for batch_start in range(0, total_movies, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_movies)\n",
    "        batch_movies = movies_with_imdb[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\" BATCH {(batch_start // batch_size) + 1}: Processing movies {batch_start + 1}-{batch_end} of {total_movies}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Scrape this batch concurrently (5 workers)\n",
    "        batch_results = []\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            futures = {executor.submit(scrape_imdb_reviews_for_movie, movie): movie \n",
    "                       for movie in batch_movies}\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                batch_results.append(future.result())\n",
    "        \n",
    "        # Save this batch to database immediately\n",
    "        db = SessionLocal()\n",
    "        batch_reviews_added = 0\n",
    "        batch_duplicates = 0\n",
    "        batch_successful = 0\n",
    "        \n",
    "        try:\n",
    "            for movie_id, movie_title, imdb_id, reviews_data, success in batch_results:\n",
    "                if success and reviews_data:\n",
    "                    # Get the movie from database using ID\n",
    "                    movie_obj = db.query(Movie).filter(Movie.id == movie_id).first()\n",
    "                    if movie_obj:\n",
    "                        for review_data in reviews_data:\n",
    "                            # Check if review already exists (prevent duplicates)\n",
    "                            existing_review = db.query(Review).filter(\n",
    "                                Review.movie_id == movie_obj.id,\n",
    "                                Review.source == 'imdb',\n",
    "                                Review.author == review_data.get('author'),\n",
    "                                Review.text == review_data.get('text')\n",
    "                            ).first()\n",
    "                            \n",
    "                            if existing_review:\n",
    "                                batch_duplicates += 1\n",
    "                                continue\n",
    "                            \n",
    "                            # Create Review object\n",
    "                            review = Review(\n",
    "                                movie_id=movie_obj.id,\n",
    "                                source='imdb',\n",
    "                                source_id=review_data.get('source_id'),\n",
    "                                text=review_data.get('text'),\n",
    "                                rating=review_data.get('rating'),\n",
    "                                author=review_data.get('author'),\n",
    "                                review_date=review_data.get('date'),\n",
    "                                helpful_count=review_data.get('helpful_count'),\n",
    "                                review_length=len(review_data.get('text', '')),\n",
    "                                word_count=len(review_data.get('text', '').split()),\n",
    "                                scraped_at=datetime.now()\n",
    "                            )\n",
    "                            db.add(review)\n",
    "                            batch_reviews_added += 1\n",
    "                        \n",
    "                        batch_successful += 1\n",
    "                        print(f\"    {movie_title}: {len(reviews_data)} reviews\")\n",
    "            \n",
    "            # Commit this batch\n",
    "            db.commit()\n",
    "            \n",
    "            # Update totals\n",
    "            total_reviews_added += batch_reviews_added\n",
    "            skipped_duplicates += batch_duplicates\n",
    "            successful_imdb_reviews += batch_successful\n",
    "            \n",
    "            print(f\"\\n Batch {(batch_start // batch_size) + 1} committed to database!\")\n",
    "            print(f\"   Movies processed: {batch_successful}/{len(batch_movies)}\")\n",
    "            print(f\"   Reviews added: {batch_reviews_added}\")\n",
    "            if batch_duplicates > 0:\n",
    "                print(f\"   Skipped duplicates: {batch_duplicates}\")\n",
    "            print(f\"\\n Running totals: {successful_imdb_reviews} movies, {total_reviews_added} reviews\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n Database error in batch {(batch_start // batch_size) + 1}: {e}\")\n",
    "            print(f\"   Error type: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            db.rollback()\n",
    "            print(\"   ‚ö†Ô∏è  Batch rolled back - continuing to next batch...\")\n",
    "        finally:\n",
    "            db.close()\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\" IMDB REVIEW SCRAPING COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\" Total movies with reviews: {successful_imdb_reviews}/{total_movies}\")\n",
    "    print(f\" Total reviews added to database: {total_reviews_added}\")\n",
    "    if skipped_duplicates > 0:\n",
    "        print(f\"‚è≠Ô∏è  Total duplicates skipped: {skipped_duplicates}\")\n",
    "    print(f\"{'='*80}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49536a",
   "metadata": {},
   "source": [
    "## Step 11b: Diagnose Failed IMDb Searches\n",
    "\n",
    "Let's identify movies that didn't get IMDb ratings and investigate why the search failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb701146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get movies that failed to get IMDb ratings\n",
    "db = SessionLocal()\n",
    "failed_movies = db.query(Movie).filter(Movie.imdb_id.is_(None)).all()\n",
    "db.close()\n",
    "\n",
    "print(f\" Movies without IMDb ratings: {len(failed_movies)}\")\n",
    "print(f\"=\" * 80)\n",
    "\n",
    "if failed_movies:\n",
    "    print(f\"\\n Sample of failed movies (first 20):\\n\")\n",
    "    for i, movie in enumerate(failed_movies[:20], 1):\n",
    "        print(f\"{i}. {movie.title} ({movie.release_year})\")\n",
    "    \n",
    "    # Analyze patterns\n",
    "    print(f\"\\n\\n Year distribution of failed movies:\")\n",
    "    from collections import Counter\n",
    "    year_counts = Counter(m.release_year for m in failed_movies if m.release_year)\n",
    "    for year in sorted(year_counts.keys(), reverse=True)[:10]:\n",
    "        print(f\"   {year}: {year_counts[year]} movies\")\n",
    "    \n",
    "    # Check for special characters or unusual titles\n",
    "    print(f\"\\n\\n Movies with special characters or long titles:\")\n",
    "    special_char_movies = [m for m in failed_movies if any(c in m.title for c in [':', '&', '-', '‚Äî', '‚Äì', '/', '(', ')'])]\n",
    "    print(f\"   Found {len(special_char_movies)} movies with special characters\")\n",
    "    for movie in special_char_movies[:10]:\n",
    "        print(f\"   - {movie.title} ({movie.release_year})\")\n",
    "else:\n",
    "    print(\" All movies have IMDb ratings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bac467",
   "metadata": {},
   "source": [
    "### IMDb Search Fallback Function\n",
    "\n",
    "This function provides a robust fallback for finding IMDb IDs when the standard scraper fails.\n",
    "\n",
    "**Features:**\n",
    "- Direct search via IMDb's `/find` endpoint (bypasses Google)\n",
    "- Parses both new and old IMDb HTML layouts\n",
    "- **Roman numeral normalization**: Converts \"Rocky II\" ‚Üí \"Rocky 2\" for better matching\n",
    "- **Fuzzy matching**: Uses `fuzzywuzzy` library to handle title variations\n",
    "- **Year validation**: Requires year to match within ¬±1 year\n",
    "- **Always returns top match**: No score threshold (returns best result found)\n",
    "\n",
    "**How it works:**\n",
    "1. Searches IMDb with the movie title and year\n",
    "2. Parses up to 10 search results\n",
    "3. Normalizes both search title and result titles (roman numerals ‚Üí numbers)\n",
    "4. Calculates fuzzy match score for each result\n",
    "5. Filters by year (¬±1 year tolerance)\n",
    "6. Returns the best matching result as a tuple: `(imdb_id, title, score)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ec02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import quote\n",
    "import time\n",
    "import re\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def normalize_title(title):\n",
    "    \"\"\"\n",
    "    Normalize title for better fuzzy matching by converting roman numerals to numbers.\n",
    "    \"\"\"\n",
    "    # Roman numeral mappings (most common in movie titles)\n",
    "    roman_map = {\n",
    "        ' I': ' 1',\n",
    "        ' II': ' 2',\n",
    "        ' III': ' 3',\n",
    "        ' IV': ' 4',\n",
    "        ' V': ' 5',\n",
    "        ' VI': ' 6',\n",
    "        ' VII': ' 7',\n",
    "        ' VIII': ' 8',\n",
    "        ' IX': ' 9',\n",
    "        ' X': ' 10',\n",
    "        ' XI': ' 11',\n",
    "        ' XII': ' 12',\n",
    "    }\n",
    "    \n",
    "    normalized = title\n",
    "    # Apply replacements in order from longest to shortest to avoid partial matches\n",
    "    for roman, number in sorted(roman_map.items(), key=lambda x: -len(x[0])):\n",
    "        # Replace at end of string or before colon/dash\n",
    "        normalized = re.sub(rf'{re.escape(roman)}(\\s*[:‚Äì-]|\\s*$)', rf'{number}\\1', normalized, flags=re.IGNORECASE)\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "def imdb_search_fallback(title, year=None):\n",
    "    \"\"\"\n",
    "    Search IMDb directly and return best matching movie using fuzzy matching.\n",
    "    \n",
    "    Args:\n",
    "        title: Movie title to search for\n",
    "        year: Optional year to help narrow down results\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (imdb_id, imdb_title, match_score) if found, None otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build search query\n",
    "        query = title\n",
    "        if year:\n",
    "            query = f\"{title} {year}\"\n",
    "        \n",
    "        # Search IMDb\n",
    "        search_url = f\"https://www.imdb.com/find/?q={quote(query)}&ref_=nv_sr_sm\"\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'\n",
    "        }\n",
    "        \n",
    "        response = requests.get(search_url, headers=headers, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Try to find results in the new IMDb layout\n",
    "        results = []\n",
    "        \n",
    "        # New layout: ipc-metadata-list-summary-item containers\n",
    "        for item in soup.find_all('li', class_='ipc-metadata-list-summary-item'):\n",
    "            try:\n",
    "                # Find the h3 element with the title\n",
    "                title_elem = item.find('h3', class_='ipc-title__text')\n",
    "                if not title_elem:\n",
    "                    continue\n",
    "                \n",
    "                result_title = title_elem.get_text(strip=True)\n",
    "                \n",
    "                # Find the IMDb ID from the link\n",
    "                link = item.find('a', class_='ipc-lockup-overlay')\n",
    "                if not link or 'href' not in link.attrs:\n",
    "                    continue\n",
    "                \n",
    "                href = link['href']\n",
    "                imdb_id_match = re.search(r'/title/(tt\\d+)', href)\n",
    "                if not imdb_id_match:\n",
    "                    continue\n",
    "                \n",
    "                imdb_id = imdb_id_match.group(1)\n",
    "                \n",
    "                # Extract year if present in metadata\n",
    "                result_year = None\n",
    "                metadata = item.find('div', class_='cli-title-metadata')\n",
    "                if metadata:\n",
    "                    year_span = metadata.find('span')\n",
    "                    if year_span:\n",
    "                        year_text = year_span.get_text(strip=True)\n",
    "                        year_match = re.search(r'\\b(19\\d{2}|20\\d{2})\\b', year_text)\n",
    "                        if year_match:\n",
    "                            result_year = year_match.group(1)\n",
    "                \n",
    "                # Also check for year in title itself\n",
    "                if not result_year:\n",
    "                    year_in_title = re.search(r'\\((\\d{4})\\)', result_title)\n",
    "                    if year_in_title:\n",
    "                        result_year = year_in_title.group(1)\n",
    "                \n",
    "                results.append({\n",
    "                    'title': result_title,\n",
    "                    'imdb_id': imdb_id,\n",
    "                    'year': result_year\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        # Fallback to old layout if new layout didn't work\n",
    "        if not results:\n",
    "            for result in soup.find_all('td', class_='result_text'):\n",
    "                try:\n",
    "                    link = result.find('a')\n",
    "                    if not link:\n",
    "                        continue\n",
    "                    \n",
    "                    result_title = link.get_text(strip=True)\n",
    "                    href = link.get('href', '')\n",
    "                    \n",
    "                    imdb_id_match = re.search(r'/title/(tt\\d+)', href)\n",
    "                    if not imdb_id_match:\n",
    "                        continue\n",
    "                    \n",
    "                    imdb_id = imdb_id_match.group(1)\n",
    "                    \n",
    "                    # Extract year from result text\n",
    "                    result_year = None\n",
    "                    year_match = re.search(r'\\((\\d{4})\\)', result.get_text())\n",
    "                    if year_match:\n",
    "                        result_year = year_match.group(1)\n",
    "                    \n",
    "                    results.append({\n",
    "                        'title': result_title,\n",
    "                        'imdb_id': imdb_id,\n",
    "                        'year': result_year\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "        \n",
    "        if not results:\n",
    "            print(f\"  No results found for '{title}'\")\n",
    "            return None\n",
    "        \n",
    "        # Normalize search title for better matching\n",
    "        normalized_search_title = normalize_title(title)\n",
    "        \n",
    "        # Now fuzzy match against the search title\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "        \n",
    "        for result in results[:10]:  # Check top 10 results\n",
    "            # Strip year from title for better matching\n",
    "            result_title = re.sub(r'\\s*\\(\\d{4}\\).*$', '', result['title'])\n",
    "            \n",
    "            # Normalize result title for comparison\n",
    "            normalized_result_title = normalize_title(result_title)\n",
    "            \n",
    "            # Calculate fuzzy match score on normalized titles\n",
    "            score = fuzz.token_sort_ratio(normalized_search_title.lower(), normalized_result_title.lower())\n",
    "            \n",
    "            # Year matching: if we have a year, require it to match (or be within 1 year)\n",
    "            year_match = True\n",
    "            if year and result['year']:\n",
    "                try:\n",
    "                    year_diff = abs(int(year) - int(result['year']))\n",
    "                    year_match = year_diff <= 1\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            print(f\"  - '{result_title}' ({result['year'] or 'N/A'}): {score}% match, year_match={year_match}\")\n",
    "            \n",
    "            # Update best match if this is better and year matches\n",
    "            if year_match and score > best_score:\n",
    "                best_score = score\n",
    "                best_match = result\n",
    "        \n",
    "        # Return best match regardless of threshold (always return top match)\n",
    "        if best_match:\n",
    "            # Clean up the title (remove year and position prefix like \"1. \")\n",
    "            clean_title = re.sub(r'^\\d+\\.\\s*', '', best_match['title'])  # Remove \"1. \" prefix\n",
    "            clean_title = re.sub(r'\\s*\\(\\d{4}\\).*$', '', clean_title)  # Remove year\n",
    "            \n",
    "            print(f\"  ‚úì Best match: '{clean_title}' ({best_match['year']}) - {best_score}% - {best_match['imdb_id']}\")\n",
    "            return (best_match['imdb_id'], clean_title, best_score)\n",
    "        else:\n",
    "            print(f\"  ‚úó No match found\")\n",
    "            return None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"  Error searching IMDb: {e}\")\n",
    "        return None\n",
    "\n",
    "# Quick test to verify function returns tuple\n",
    "print(\" Function loaded successfully!\")\n",
    "print(\" Testing return format...\")\n",
    "test_result = imdb_search_fallback(\"The Love Witch\", 2016)\n",
    "if test_result:\n",
    "    print(f\" Returns tuple: {type(test_result)} with {len(test_result)} elements\")\n",
    "    print(f\"   IMDb ID: {test_result[0]}\")\n",
    "    print(f\"   Title: {test_result[1]}\")\n",
    "    print(f\"   Score: {test_result[2]}%\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No result found (None returned)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751bfbb9",
   "metadata": {},
   "source": [
    "### Apply IMDb Search to All Failed Movies\n",
    "\n",
    "Now let's run the IMDb search method on ALL movies that failed the standard IMDb search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef58132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply IMDb search to ALL failed movies\n",
    "from sqlalchemy.exc import OperationalError\n",
    "\n",
    "db = SessionLocal()\n",
    "all_failed_movies = db.query(Movie).filter(Movie.imdb_id.is_(None)).all()\n",
    "\n",
    "# Store movie IDs and details to avoid detached instance errors\n",
    "movie_data_list = [(m.id, m.title, m.release_year) for m in all_failed_movies]\n",
    "db.close()\n",
    "\n",
    "print(f\" Attempting IMDb search for {len(movie_data_list)} failed movies\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "successful_recoveries = 0\n",
    "still_failed = 0\n",
    "skipped_duplicates = 0\n",
    "\n",
    "for i, (movie_id, title, year) in enumerate(movie_data_list, 1):\n",
    "    print(f\"\\n[{i}/{len(movie_data_list)}] Searching: {title} ({year})\")\n",
    "    \n",
    "    try:\n",
    "        # Returns tuple (imdb_id, imdb_title, match_score) or None\n",
    "        search_result = imdb_search_fallback(title, year)\n",
    "        \n",
    "        if search_result:\n",
    "            imdb_id, imdb_title, match_score = search_result\n",
    "            \n",
    "            # Open fresh session for each update\n",
    "            db = SessionLocal()\n",
    "            \n",
    "            try:\n",
    "                # Check if this IMDb ID is already assigned to another movie\n",
    "                existing_movie = db.query(Movie).filter(Movie.imdb_id == imdb_id).first()\n",
    "                \n",
    "                if existing_movie and existing_movie.id != movie_id:\n",
    "                    print(f\"‚ö†Ô∏è  IMDb ID {imdb_id} already assigned to '{existing_movie.title}' (ID: {existing_movie.id})\")\n",
    "                    print(f\"   Skipping to avoid duplicate\")\n",
    "                    skipped_duplicates += 1\n",
    "                else:\n",
    "                    # Fetch and update the movie (no conflict)\n",
    "                    movie = db.query(Movie).filter(Movie.id == movie_id).first()\n",
    "                    if movie:\n",
    "                        movie.imdb_id = imdb_id\n",
    "                        db.commit()\n",
    "                        successful_recoveries += 1\n",
    "                        print(f\" Updated database with {imdb_id}\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è  Movie ID {movie_id} not found in database\")\n",
    "                        still_failed += 1\n",
    "            \n",
    "            except OperationalError as e:\n",
    "                print(f\"‚ö†Ô∏è  Database error: {e}\")\n",
    "                still_failed += 1\n",
    "            \n",
    "            finally:\n",
    "                db.close()\n",
    "        else:\n",
    "            still_failed += 1\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" Unexpected error: {e}\")\n",
    "        still_failed += 1\n",
    "    \n",
    "    # Rate limiting - be respectful to IMDb\n",
    "    if i < len(movie_data_list):\n",
    "        time.sleep(1.5)  # 1.5 seconds between searches\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" IMDB SEARCH RECOVERY SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\" Successfully recovered: {successful_recoveries} movies\")\n",
    "print(f\"‚ö†Ô∏è  Skipped duplicates: {skipped_duplicates} movies\")\n",
    "print(f\" Still without IMDb ID: {still_failed} movies\")\n",
    "if len(movie_data_list) > 0:\n",
    "    print(f\" Recovery rate: {(successful_recoveries / len(movie_data_list) * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e6c17",
   "metadata": {},
   "source": [
    "### Scrape Data for Recovered Movies\n",
    "\n",
    "For movies that were successfully recovered via Google search, now scrape their ratings and reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get movies that now have IMDb IDs but no ratings yet\n",
    "db = SessionLocal()\n",
    "recovered_movies = db.query(Movie).filter(\n",
    "    Movie.imdb_id.isnot(None),\n",
    "    Movie.imdb_rating.is_(None)\n",
    ").all()\n",
    "\n",
    "if not recovered_movies:\n",
    "    print(\" All movies with IMDb IDs already have ratings!\")\n",
    "    db.close()\n",
    "else:\n",
    "    print(f\" Scraping data for {len(recovered_movies)} recovered movies\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    \n",
    "    # Scrape ratings\n",
    "    def scrape_rating_for_recovered(movie_id):\n",
    "        db_local = SessionLocal()\n",
    "        try:\n",
    "            movie = db_local.query(Movie).filter(Movie.id == movie_id).first()\n",
    "            if not movie:\n",
    "                return movie_id, None, None, False\n",
    "            \n",
    "            imdb_data = imdb.scrape_movie_rating(movie.title, movie.release_year, movie.imdb_id)\n",
    "            \n",
    "            if imdb_data and 'rating' in imdb_data:\n",
    "                movie.imdb_rating = imdb_data['rating']\n",
    "                movie.imdb_rating_count = imdb_data.get('rating_count')\n",
    "                movie.scraped_at = datetime.now()\n",
    "                db_local.commit()\n",
    "                return movie_id, movie.title, imdb_data, True\n",
    "            return movie_id, movie.title, None, False\n",
    "        finally:\n",
    "            db_local.close()\n",
    "    \n",
    "    # Scrape ratings concurrently\n",
    "    print(\"\\n Scraping IMDb ratings for recovered movies...\")\n",
    "    successful_ratings = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = {executor.submit(scrape_rating_for_recovered, m.id): m for m in recovered_movies}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            movie_id, title, imdb_data, success = future.result()\n",
    "            if success:\n",
    "                successful_ratings += 1\n",
    "                rating = imdb_data.get('rating', 'N/A')\n",
    "                print(f\"    {title}: {rating}/10\")\n",
    "    \n",
    "    print(f\"\\n Scraped ratings for {successful_ratings}/{len(recovered_movies)} recovered movies\")\n",
    "    \n",
    "    # Now scrape reviews for these movies\n",
    "    print(\"\\n Scraping IMDb reviews for recovered movies...\")\n",
    "    \n",
    "    def scrape_reviews_for_recovered(movie_id):\n",
    "        db_local = SessionLocal()\n",
    "        try:\n",
    "            movie = db_local.query(Movie).filter(Movie.id == movie_id).first()\n",
    "            if not movie or not movie.imdb_id:\n",
    "                return 0\n",
    "            \n",
    "            reviews_data = imdb.scrape_reviews(movie.imdb_id, max_reviews=30)\n",
    "            reviews_added = 0\n",
    "            \n",
    "            for review_data in reviews_data:\n",
    "                existing = db_local.query(Review).filter(\n",
    "                    Review.movie_id == movie.id,\n",
    "                    Review.source == 'imdb',\n",
    "                    Review.text == review_data.get('text')\n",
    "                ).first()\n",
    "                \n",
    "                if not existing:\n",
    "                    review = Review(\n",
    "                        movie_id=movie.id,\n",
    "                        source='imdb',\n",
    "                        author=review_data.get('author'),\n",
    "                        rating=review_data.get('rating'),\n",
    "                        text=review_data.get('text'),\n",
    "                        review_date=review_data.get('date'),\n",
    "                        helpful_count=review_data.get('helpful_count'),\n",
    "                        not_helpful_count=review_data.get('not_helpful_count'),\n",
    "                        review_length=review_data.get('review_length'),\n",
    "                        word_count=review_data.get('word_count')\n",
    "                    )\n",
    "                    db_local.add(review)\n",
    "                    reviews_added += 1\n",
    "            \n",
    "            db_local.commit()\n",
    "            return reviews_added\n",
    "        finally:\n",
    "            db_local.close()\n",
    "    \n",
    "    total_reviews_recovered = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        futures = {executor.submit(scrape_reviews_for_recovered, m.id): m for m in recovered_movies}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            reviews_added = future.result()\n",
    "            total_reviews_recovered += reviews_added\n",
    "    \n",
    "    print(f\" Added {total_reviews_recovered} reviews from recovered movies\")\n",
    "    \n",
    "    db.close()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\" RECOVERY COMPLETE!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57898897",
   "metadata": {},
   "source": [
    "## Step 12: Scrape Rotten Tomatoes Tomatometer Scores\n",
    "\n",
    "Get the official RT critics score (Tomatometer) for each movie and store it in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a29afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Checking database: /Users/rachitasaini/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys/data/database/movie_recommender.db\n",
      "‚úÖ rt_slug column already exists in database!\n",
      "   No migration needed - ready to scrape RT data.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if rt_slug column exists, run migration if needed\n",
    "import sqlite3\n",
    "import subprocess\n",
    "\n",
    "# Use the project_root already defined in Cell 3 (don't redefine it!)\n",
    "db_path = project_root / \"data\" / \"database\" / \"movie_recommender.db\"\n",
    "\n",
    "print(f\" Checking database: {db_path}\")\n",
    "\n",
    "# Check if rt_slug column exists\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Get table info\n",
    "cursor.execute(\"PRAGMA table_info(movies)\")\n",
    "columns = cursor.fetchall()\n",
    "column_names = [col[1] for col in columns]\n",
    "\n",
    "conn.close()\n",
    "\n",
    "if 'rt_slug' not in column_names:\n",
    "    print(\"‚ö†Ô∏è  rt_slug column not found in database!\")\n",
    "    print(\" Running migration to add rt_slug column...\")\n",
    "    print()\n",
    "    \n",
    "    # Run the migration script\n",
    "    migration_script = project_root / \"add_rt_slug_column.py\"\n",
    "    \n",
    "    if migration_script.exists():\n",
    "        result = subprocess.run(\n",
    "            [\"python3\", str(migration_script)],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            cwd=str(project_root)\n",
    "        )\n",
    "        \n",
    "        print(result.stdout)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\" Migration completed successfully!\")\n",
    "        else:\n",
    "            print(\" Migration failed!\")\n",
    "            print(result.stderr)\n",
    "    else:\n",
    "        print(f\" Migration script not found at: {migration_script}\")\n",
    "        print(\"‚ö†Ô∏è  Please run the migration manually or create the script first.\")\n",
    "else:\n",
    "    print(\" rt_slug column already exists in database!\")\n",
    "    print(\"   No migration needed - ready to scrape RT data.\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdbeb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä RT Tomatometer Status:\n",
      "   Total movies in database: 2,187\n",
      "   Already have RT scores: 573\n",
      "   Need RT scores: 1,614\n",
      "\n",
      "üé¨ Found 1,614 movies to scrape for RT Tomatometer scores\n",
      "\n",
      "2025-12-08 15:16:48 - scrapers.rotten_tomatoes_selenium - INFO - Initializing Rotten Tomatoes Selenium scraper\n",
      "üì¶ Processing batch 1/162 (1-10/1614)\n",
      "üìΩÔ∏è  A Silent Voice: The Movie (2016)\n",
      "2025-12-08 15:16:48 - scrapers.rotten_tomatoes_selenium - INFO - üîç Strategy 1: Searching without year for 'A Silent Voice: The Movie'\n",
      "2025-12-08 15:16:48 - scrapers.rotten_tomatoes_selenium - INFO - Setting up Chrome WebDriver...\n",
      "2025-12-08 15:16:49 - scrapers.rotten_tomatoes_selenium - INFO - ‚úÖ Chrome WebDriver initialized\n",
      "2025-12-08 15:16:49 - scrapers.rotten_tomatoes_selenium - INFO - Searching RT: https://www.rottentomatoes.com/search?search=A%20Silent%20Voice%3A%20The%20Movie\n",
      "2025-12-08 15:16:50 - scrapers.rotten_tomatoes_selenium - INFO - Found 11 search results for 'A Silent Voice: The Movie'\n",
      "2025-12-08 15:16:51 - scrapers.rotten_tomatoes_selenium - WARNING - ‚ö†Ô∏è Best match score too low (0.49): 'A Silent Voice' for 'A Silent Voice: The Movie'\n",
      "2025-12-08 15:16:51 - scrapers.rotten_tomatoes_selenium - INFO - üîç Strategy 2: Searching with year for 'A Silent Voice: The Movie' (2016)\n",
      "2025-12-08 15:16:51 - scrapers.rotten_tomatoes_selenium - INFO - Searching RT: https://www.rottentomatoes.com/search?search=A%20Silent%20Voice%3A%20The%20Movie\n",
      "2025-12-08 15:16:52 - scrapers.rotten_tomatoes_selenium - INFO - Found 11 search results for 'A Silent Voice: The Movie'\n",
      "2025-12-08 15:16:52 - scrapers.rotten_tomatoes_selenium - WARNING - ‚ö†Ô∏è Best match score too low (0.49): 'A Silent Voice' for 'A Silent Voice: The Movie'\n",
      "2025-12-08 15:16:52 - scrapers.rotten_tomatoes_selenium - WARNING - ‚ö†Ô∏è Search failed, trying slug generation without year\n",
      "2025-12-08 15:16:52 - scrapers.rotten_tomatoes_selenium - INFO - Generated slug (no year): a_silent_voice_the_movie\n",
      "2025-12-08 15:16:52 - scrapers.rotten_tomatoes_selenium - INFO - Getting RT score from: https://www.rottentomatoes.com/m/a_silent_voice_the_movie\n",
      "2025-12-08 15:16:54 - scrapers.rotten_tomatoes_selenium - INFO - Tomatometer shows '- -', trying Popcornmeter...\n",
      "2025-12-08 15:16:54 - scrapers.rotten_tomatoes_selenium - INFO - Tomatometer not available, trying Popcornmeter...\n",
      "2025-12-08 15:16:54 - scrapers.rotten_tomatoes_selenium - WARNING - ‚ö†Ô∏è  Neither Tomatometer nor Popcornmeter found for: a_silent_voice_the_movie\n",
      "   ‚ö†Ô∏è  Could not find RT score\n",
      "üìΩÔ∏è  Good Guys Go to Heaven, Bad Guys Go to Pattaya (2016)\n",
      "2025-12-08 15:16:54 - scrapers.rotten_tomatoes_selenium - INFO - üîç Strategy 1: Searching without year for 'Good Guys Go to Heaven, Bad Guys Go to Pattaya'\n",
      "2025-12-08 15:16:54 - scrapers.rotten_tomatoes_selenium - INFO - Searching RT: https://www.rottentomatoes.com/search?search=Good%20Guys%20Go%20to%20Heaven%2C%20Bad%20Guys%20Go%20to%20Pattaya\n",
      "2025-12-08 15:16:55 - scrapers.rotten_tomatoes_selenium - INFO - Found 11 search results for 'Good Guys Go to Heaven, Bad Guys Go to Pattaya'\n",
      "2025-12-08 15:16:56 - scrapers.rotten_tomatoes_selenium - INFO - ‚úÖ Match found: 'Good Guys Go to Heaven, Bad Guys Go to Pattaya' (None) -> all_good_junkies_go_to_heaven [score: 1.00]\n",
      "2025-12-08 15:16:56 - scrapers.rotten_tomatoes_selenium - INFO - ‚úÖ Found via search (no year): all_good_junkies_go_to_heaven\n",
      "2025-12-08 15:16:56 - scrapers.rotten_tomatoes_selenium - INFO - Getting RT score from: https://www.rottentomatoes.com/m/all_good_junkies_go_to_heaven\n",
      "2025-12-08 15:16:57 - scrapers.rotten_tomatoes_selenium - INFO - Tomatometer shows '- -', trying Popcornmeter...\n",
      "2025-12-08 15:16:57 - scrapers.rotten_tomatoes_selenium - INFO - Tomatometer not available, trying Popcornmeter...\n",
      "2025-12-08 15:16:57 - scrapers.rotten_tomatoes_selenium - WARNING - ‚ö†Ô∏è  Neither Tomatometer nor Popcornmeter available for: all_good_junkies_go_to_heaven\n",
      "   ‚ö†Ô∏è  Could not find RT score\n",
      "üìΩÔ∏è  Don't Blame the Kid (2016)\n",
      "2025-12-08 15:16:57 - scrapers.rotten_tomatoes_selenium - INFO - üîç Strategy 1: Searching without year for 'Don't Blame the Kid'\n",
      "2025-12-08 15:16:57 - scrapers.rotten_tomatoes_selenium - INFO - Searching RT: https://www.rottentomatoes.com/search?search=Don%27t%20Blame%20the%20Kid\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RottenTomatoesSeleniumScraper' object has no attribute 'close'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     movie_slug = \u001b[43mrt_scraper\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch_movie\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmovie\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmovie\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelease_year\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m movie_slug:\n\u001b[32m     64\u001b[39m         \u001b[38;5;66;03m# Save the slug for future use (e.g., review scraping)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys/src/scrapers/rotten_tomatoes_selenium.py:288\u001b[39m, in \u001b[36mRottenTomatoesSeleniumScraper.search_movie\u001b[39m\u001b[34m(self, title, year)\u001b[39m\n\u001b[32m    287\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müîç Strategy 1: Searching without year for \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m slug = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_search_via_selenium\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myear\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m slug:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys/src/scrapers/rotten_tomatoes_selenium.py:332\u001b[39m, in \u001b[36mRottenTomatoesSeleniumScraper._search_via_selenium\u001b[39m\u001b[34m(self, title, year)\u001b[39m\n\u001b[32m    331\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSearching RT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43msearch_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[38;5;66;03m# Wait for search results to load (increased timeout)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys/venv/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:452\u001b[39m, in \u001b[36mWebDriver.get\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Navigate the browser to the specified URL.\u001b[39;00m\n\u001b[32m    441\u001b[39m \n\u001b[32m    442\u001b[39m \u001b[33;03mThe method does not return until the page is fully loaded (i.e. the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m \u001b[33;03m    `driver.get(\"https://example.com\")`\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys/venv/lib/python3.11/site-packages/selenium/webdriver/remote/webdriver.py:429\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    427\u001b[39m         params[\u001b[33m\"\u001b[39m\u001b[33msessionId\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.session_id\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m response = \u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRemoteConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcommand_executor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys/venv/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:405\u001b[39m, in \u001b[36mRemoteConnection.execute\u001b[39m\u001b[34m(self, command, params)\u001b[39m\n\u001b[32m    404\u001b[39m LOGGER.debug(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, command_info[\u001b[32m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys/venv/lib/python3.11/site-packages/selenium/webdriver/remote/remote_connection.py:429\u001b[39m, in \u001b[36mRemoteConnection._request\u001b[39m\u001b[34m(self, method, url, body)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client_config.keep_alive:\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    430\u001b[39m     statuscode = response.status\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys/venv/lib/python3.11/site-packages/urllib3/_request_methods.py:143\u001b[39m, in \u001b[36mRequestMethods.request\u001b[39m\u001b[34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43murlopen_kw\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys/venv/lib/python3.11/site-packages/urllib3/_request_methods.py:278\u001b[39m, in \u001b[36mRequestMethods.request_encode_body\u001b[39m\u001b[34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[39m\n\u001b[32m    276\u001b[39m extra_kw.update(urlopen_kw)\n\u001b[32m--> \u001b[39m\u001b[32m278\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys/venv/lib/python3.11/site-packages/urllib3/poolmanager.py:457\u001b[39m, in \u001b[36mPoolManager.urlopen\u001b[39m\u001b[34m(self, method, url, redirect, **kw)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    459\u001b[39m redirect_location = redirect \u001b[38;5;129;01mand\u001b[39;00m response.get_redirect_location()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys/venv/lib/python3.11/site-packages/urllib3/connectionpool.py:534\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m534\u001b[39m     response = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Rutgers/Fall 2026/Intro to Data Science 01-198-439/project/hybrid-rec-sys/venv/lib/python3.11/site-packages/urllib3/connection.py:571\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    570\u001b[39m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m httplib_response = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1374\u001b[39m, in \u001b[36mHTTPConnection.getresponse\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1373\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1374\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:318\u001b[39m, in \u001b[36mHTTPResponse.begin\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m318\u001b[39m     version, status, reason = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m status != CONTINUE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:279\u001b[39m, in \u001b[36mHTTPResponse._read_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     line = \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m.fp.readline(_MAXLINE + \u001b[32m1\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    280\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     98\u001b[39m             \u001b[38;5;28mprint\u001b[39m()\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    101\u001b[39m     \u001b[38;5;66;03m# Clean up\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     \u001b[43mrt_scraper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m()\n\u001b[32m    103\u001b[39m     session.close()\n\u001b[32m    105\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'RottenTomatoesSeleniumScraper' object has no attribute 'close'"
     ]
    }
   ],
   "source": [
    "# Import the Rotten Tomatoes scraper\n",
    "from scrapers.rotten_tomatoes_selenium import RottenTomatoesSeleniumScraper\n",
    "from database.models import Movie\n",
    "from sqlalchemy import func\n",
    "\n",
    "# Create a session\n",
    "session = SessionLocal()\n",
    "\n",
    "# Get total counts\n",
    "total_movies = session.query(Movie).count()\n",
    "already_scraped = session.query(Movie).filter(Movie.rt_tomatometer.isnot(None)).count()\n",
    "needs_scraping = total_movies - already_scraped\n",
    "\n",
    "print(f\" RT Tomatometer Status:\")\n",
    "print(f\"   Total movies in database: {total_movies:,}\")\n",
    "print(f\"   Already have RT scores: {already_scraped:,}\")\n",
    "print(f\"   Need RT scores: {needs_scraping:,}\")\n",
    "print()\n",
    "\n",
    "# Only proceed if there are movies to scrape\n",
    "if needs_scraping == 0:\n",
    "    print(\" All movies already have RT Tomatometer scores! Nothing to scrape.\")\n",
    "else:\n",
    "    # Get movies that don't have RT Tomatometer scores yet (ignore rt_slug for now)\n",
    "    movies_to_scrape = session.query(Movie).filter(\n",
    "        Movie.rt_tomatometer.is_(None)\n",
    "    ).all()\n",
    "\n",
    "    print(f\" Found {len(movies_to_scrape):,} movies to scrape for RT Tomatometer scores\")\n",
    "    print()\n",
    "\n",
    "    # Initialize the scraper\n",
    "    rt_scraper = RottenTomatoesSeleniumScraper(headless=True)\n",
    "\n",
    "    # Batch settings\n",
    "    BATCH_SIZE = 10\n",
    "    total_movies = len(movies_to_scrape)\n",
    "    total_batches = (total_movies + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "    # Counters\n",
    "    total_successful = 0\n",
    "    total_failed = 0\n",
    "\n",
    "    try:\n",
    "        # Process in batches\n",
    "        for batch_num in range(total_batches):\n",
    "            start_idx = batch_num * BATCH_SIZE\n",
    "            end_idx = min(start_idx + BATCH_SIZE, total_movies)\n",
    "            batch_movies = movies_to_scrape[start_idx:end_idx]\n",
    "            \n",
    "            print(f\" Processing batch {batch_num + 1}/{total_batches} ({start_idx + 1}-{end_idx}/{total_movies})\")\n",
    "            \n",
    "            batch_successful = 0\n",
    "            batch_failed = 0\n",
    "            \n",
    "            # Process each movie in the batch\n",
    "            for movie in batch_movies:\n",
    "                print(f\"üìΩÔ∏è  {movie.title} ({movie.release_year})\")\n",
    "                \n",
    "                try:\n",
    "                    movie_slug = rt_scraper.search_movie(movie.title, movie.release_year)\n",
    "                    \n",
    "                    if movie_slug:\n",
    "                        # Save the slug for future use (e.g., review scraping)\n",
    "                        movie.rt_slug = movie_slug\n",
    "                        \n",
    "                        rt_score = rt_scraper.get_tomatometer_score(movie_slug)\n",
    "                        \n",
    "                        if rt_score is not None:\n",
    "                            rt_score_out_of_10 = round(rt_score / 10.0, 2)\n",
    "                            movie.rt_tomatometer = rt_score\n",
    "                            movie.rt_tomatometer_out_of_10 = rt_score_out_of_10\n",
    "                            \n",
    "                            print(f\"    RT Score: {rt_score}% (out of 10: {rt_score_out_of_10})\")\n",
    "                            batch_successful += 1\n",
    "                        else:\n",
    "                            print(f\"   ‚ö†Ô∏è  Could not find RT score\")\n",
    "                            batch_failed += 1\n",
    "                    else:\n",
    "                        print(f\"   ‚ö†Ô∏è  Could not find movie on RT\")\n",
    "                        batch_failed += 1\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"    Error: {str(e)[:100]}\")\n",
    "                    batch_failed += 1\n",
    "                    continue\n",
    "            \n",
    "            # Commit this batch\n",
    "            try:\n",
    "                session.commit()\n",
    "                total_successful += batch_successful\n",
    "                total_failed += batch_failed\n",
    "                print(f\" Batch {batch_num + 1} committed: {batch_successful} successful, {batch_failed} failed\")\n",
    "                print()\n",
    "            except Exception as e:\n",
    "                session.rollback()\n",
    "                print(f\" Failed to commit batch {batch_num + 1}: {str(e)}\")\n",
    "                print()\n",
    "    \n",
    "    finally:\n",
    "        # Clean up\n",
    "        rt_scraper.close()\n",
    "        session.close()\n",
    "        \n",
    "    print(\"=\" * 80)\n",
    "    print(f\" RT Tomatometer Scraping Complete!\")\n",
    "    print(f\"    Successfully scraped: {total_successful:,}\")\n",
    "    print(f\"    Failed to scrape: {total_failed:,}\")\n",
    "    print(f\"    Success rate: {(total_successful / (total_successful + total_failed) * 100) if (total_successful + total_failed) > 0 else 0:.1f}%\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97088ca9",
   "metadata": {},
   "source": [
    "## Step 13: Scrape Rotten Tomatoes Reviews\n",
    "\n",
    "Scrape user reviews from Rotten Tomatoes for all movies (with or without Tomatometer scores).\n",
    "Reviews are collected from 4 endpoints: top-critics, all-critics, verified-audience, and all-audience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeb6a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\" SCRAPING ROTTEN TOMATOES REVIEWS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from scrapers.rotten_tomatoes_selenium import RottenTomatoesSeleniumScraper\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize RT scraper\n",
    "rt_scraper = RottenTomatoesSeleniumScraper(headless=True)\n",
    "\n",
    "# Get ALL movies that don't have RT reviews yet (regardless of whether they have scores)\n",
    "db = SessionLocal()\n",
    "\n",
    "# Subquery to find movies that already have RT reviews\n",
    "movies_with_rt_reviews_subquery = db.query(Review.movie_id).filter(\n",
    "    Review.source == 'rotten_tomatoes'\n",
    ").distinct().subquery()\n",
    "\n",
    "# Get ALL movies that are NOT in the subquery (don't have RT reviews yet)\n",
    "# This includes movies with AND without RT Tomatometer scores\n",
    "movies_needing_reviews = db.query(Movie).filter(\n",
    "    ~Movie.id.in_(db.query(movies_with_rt_reviews_subquery.c.movie_id))\n",
    ").all()\n",
    "\n",
    "# Also count how many already have reviews\n",
    "movies_already_scraped = db.query(Movie).filter(\n",
    "    Movie.id.in_(db.query(movies_with_rt_reviews_subquery.c.movie_id))\n",
    ").count()\n",
    "\n",
    "# Count how many have RT scores vs. don't have RT scores\n",
    "movies_with_scores = sum(1 for m in movies_needing_reviews if m.rt_tomatometer is not None)\n",
    "movies_without_scores = len(movies_needing_reviews) - movies_with_scores\n",
    "\n",
    "# Count how many already have rt_slug saved (can skip RT search)\n",
    "movies_with_slug_saved = sum(1 for m in movies_needing_reviews if m.rt_slug is not None)\n",
    "movies_need_slug_search = len(movies_needing_reviews) - movies_with_slug_saved\n",
    "\n",
    "db.close()\n",
    "\n",
    "if not movies_needing_reviews:\n",
    "    print(\"‚ö†Ô∏è  No movies need RT review scraping!\")\n",
    "    if movies_already_scraped > 0:\n",
    "        print(f\"‚ÑπÔ∏è  All {movies_already_scraped} movies already have RT reviews\")\n",
    "else:\n",
    "    print(f\" Found {len(movies_needing_reviews)} movies needing RT reviews\")\n",
    "    print(f\"   ‚Ä¢ {movies_with_scores} movies have RT Tomatometer scores\")\n",
    "    print(f\"   ‚Ä¢ {movies_without_scores} movies don't have RT scores (will search for RT page)\")\n",
    "    print(f\"   ‚Ä¢ {movies_with_slug_saved} movies have RT slug saved (no search needed)\")\n",
    "    print(f\"   ‚Ä¢ {movies_need_slug_search} movies need RT page search\")\n",
    "    if movies_already_scraped > 0:\n",
    "        print(f\" Skipping {movies_already_scraped} movies that already have RT reviews\\n\")\n",
    "    \n",
    "    # Statistics\n",
    "    successful_rt_reviews = 0\n",
    "    total_reviews_added = 0\n",
    "    skipped_duplicates = 0\n",
    "    movies_with_slug_found = 0\n",
    "    movies_with_slug_not_found = 0\n",
    "    slug_searches_performed = 0\n",
    "    slug_searches_skipped = 0\n",
    "    \n",
    "    def scrape_rt_reviews_for_movie(movie):\n",
    "        \"\"\"Scrape RT reviews for a single movie\"\"\"\n",
    "        nonlocal slug_searches_performed, slug_searches_skipped\n",
    "        \n",
    "        try:\n",
    "            # Check if we already have the RT slug saved\n",
    "            if movie.rt_slug:\n",
    "                movie_slug = movie.rt_slug\n",
    "                slug_searches_skipped += 1\n",
    "            else:\n",
    "                # Search for movie slug (works for all movies, not just those with scores)\n",
    "                movie_slug = rt_scraper.search_movie(movie.title, movie.release_year)\n",
    "                slug_searches_performed += 1\n",
    "            \n",
    "            if movie_slug:\n",
    "                # Scrape reviews (max 30 per movie)\n",
    "                reviews_data = rt_scraper.scrape_reviews(\n",
    "                    movie_slug=movie_slug,\n",
    "                    max_reviews=30\n",
    "                )\n",
    "                \n",
    "                if reviews_data:\n",
    "                    return (movie.id, movie.title, movie_slug, reviews_data, True, True)\n",
    "                else:\n",
    "                    # Found slug but no reviews\n",
    "                    return (movie.id, movie.title, movie_slug, [], False, True)\n",
    "            else:\n",
    "                # Could not find RT page for this movie\n",
    "                return (movie.id, movie.title, None, [], False, False)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"    {movie.title}: Error - {str(e)[:100]}\")\n",
    "            return (movie.id, movie.title, None, [], False, False)\n",
    "    \n",
    "    # Process in batches of 10\n",
    "    batch_size = 10\n",
    "    total_movies = len(movies_needing_reviews)\n",
    "    \n",
    "    for batch_start in range(0, total_movies, batch_size):\n",
    "        batch_end = min(batch_start + batch_size, total_movies)\n",
    "        batch_movies = movies_needing_reviews[batch_start:batch_end]\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\" BATCH {(batch_start // batch_size) + 1}: Processing movies {batch_start + 1}-{batch_end} of {total_movies}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # Scrape this batch concurrently (3 workers for RT)\n",
    "        batch_results = []\n",
    "        with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "            futures = {executor.submit(scrape_rt_reviews_for_movie, movie): movie \n",
    "                       for movie in batch_movies}\n",
    "            \n",
    "            for future in as_completed(futures):\n",
    "                batch_results.append(future.result())\n",
    "        \n",
    "        # Save this batch to database immediately\n",
    "        db = SessionLocal()\n",
    "        batch_reviews_added = 0\n",
    "        batch_duplicates = 0\n",
    "        batch_successful = 0\n",
    "        batch_slug_found = 0\n",
    "        batch_slug_not_found = 0\n",
    "        \n",
    "        try:\n",
    "            for movie_id, movie_title, movie_slug, reviews_data, has_reviews, slug_found in batch_results:\n",
    "                # Track slug finding rate\n",
    "                if slug_found:\n",
    "                    batch_slug_found += 1\n",
    "                    movies_with_slug_found += 1\n",
    "                else:\n",
    "                    batch_slug_not_found += 1\n",
    "                    movies_with_slug_not_found += 1\n",
    "                \n",
    "                if has_reviews and reviews_data:\n",
    "                    # Get the movie from database using ID\n",
    "                    movie_obj = db.query(Movie).filter(Movie.id == movie_id).first()\n",
    "                    if movie_obj:\n",
    "                        for review_data in reviews_data:\n",
    "                            # Check if review already exists (prevent duplicates)\n",
    "                            existing_review = db.query(Review).filter(\n",
    "                                Review.movie_id == movie_obj.id,\n",
    "                                Review.source == 'rotten_tomatoes',\n",
    "                                Review.author == review_data.get('author'),\n",
    "                                Review.text == review_data.get('text')\n",
    "                            ).first()\n",
    "                            \n",
    "                            if existing_review:\n",
    "                                batch_duplicates += 1\n",
    "                                continue\n",
    "                            \n",
    "                            # Create Review object\n",
    "                            review = Review(\n",
    "                                movie_id=movie_obj.id,\n",
    "                                source='rotten_tomatoes',\n",
    "                                source_id=review_data.get('source_id'),\n",
    "                                text=review_data.get('text'),\n",
    "                                rating=review_data.get('rating'),\n",
    "                                author=review_data.get('author'),\n",
    "                                review_date=review_data.get('date'),\n",
    "                                helpful_count=review_data.get('helpful_count'),\n",
    "                                review_length=len(review_data.get('text', '')),\n",
    "                                word_count=len(review_data.get('text', '').split()),\n",
    "                                scraped_at=datetime.now()\n",
    "                            )\n",
    "                            db.add(review)\n",
    "                            batch_reviews_added += 1\n",
    "                        \n",
    "                        batch_successful += 1\n",
    "                        print(f\"    {movie_title}: {len(reviews_data)} reviews\")\n",
    "                elif slug_found and not has_reviews:\n",
    "                    print(f\"   ‚ö†Ô∏è  {movie_title}: RT page found but no reviews available\")\n",
    "                else:\n",
    "                    print(f\"    {movie_title}: RT page not found\")\n",
    "            \n",
    "            # Commit this batch\n",
    "            db.commit()\n",
    "            \n",
    "            # Update totals\n",
    "            total_reviews_added += batch_reviews_added\n",
    "            skipped_duplicates += batch_duplicates\n",
    "            successful_rt_reviews += batch_successful\n",
    "            \n",
    "            print(f\"\\n Batch {(batch_start // batch_size) + 1} committed to database!\")\n",
    "            print(f\"   Movies with reviews: {batch_successful}/{len(batch_movies)}\")\n",
    "            print(f\"   Reviews added: {batch_reviews_added}\")\n",
    "            print(f\"   RT pages found: {batch_slug_found}/{len(batch_movies)}\")\n",
    "            print(f\"   RT pages not found: {batch_slug_not_found}/{len(batch_movies)}\")\n",
    "            if batch_duplicates > 0:\n",
    "                print(f\"   Skipped duplicates: {batch_duplicates}\")\n",
    "            print(f\"\\n Running totals: {successful_rt_reviews} movies with reviews, {total_reviews_added} total reviews\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n Database error in batch {(batch_start // batch_size) + 1}: {e}\")\n",
    "            print(f\"   Error type: {type(e).__name__}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            db.rollback()\n",
    "            print(\"    Batch rolled back - continuing to next batch...\")\n",
    "        finally:\n",
    "            db.close()\n",
    "    \n",
    "    # Final summary and cleanup\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\" RT REVIEW SCRAPING COMPLETE!\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\" Total movies with reviews: {successful_rt_reviews}/{total_movies}\")\n",
    "    print(f\" Total reviews added to database: {total_reviews_added}\")\n",
    "    print(f\" RT pages found: {movies_with_slug_found}/{total_movies}\")\n",
    "    print(f\" RT pages not found: {movies_with_slug_not_found}/{total_movies}\")\n",
    "    if skipped_duplicates > 0:\n",
    "        print(f\"‚ö†Ô∏è  Skipped duplicate reviews: {skipped_duplicates}\")\n",
    "    print(f\"\\n Performance:\")\n",
    "    print(f\"   RT searches performed: {slug_searches_performed}\")\n",
    "    print(f\"   RT searches skipped (used saved slug): {slug_searches_skipped}\")\n",
    "    if slug_searches_skipped > 0:\n",
    "        saved_percent = (slug_searches_skipped / (slug_searches_performed + slug_searches_skipped)) * 100\n",
    "        print(f\"   Performance improvement: {saved_percent:.1f}% fewer searches!\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "# Close the scraper\n",
    "rt_scraper.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a142d",
   "metadata": {},
   "source": [
    "## Step 14: VADER Sentiment Analysis on Reviews\n",
    "\n",
    "Apply VADER sentiment analysis to all scraped reviews (IMDb + Rotten Tomatoes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"VADER SENTIMENT ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Install vaderSentiment if not already installed\n",
    "try:\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "except ImportError:\n",
    "    print(\"Installing vaderSentiment...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"vaderSentiment\"])\n",
    "    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize VADER\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Reconnect to database\n",
    "db = SessionLocal()\n",
    "\n",
    "try:\n",
    "    # Get all reviews that need sentiment analysis\n",
    "    reviews_to_analyze = db.query(Review).filter(Review.sentiment_score.is_(None)).all()\n",
    "    \n",
    "    print(f\"\\nFound {len(reviews_to_analyze)} reviews to analyze\")\n",
    "    \n",
    "    if len(reviews_to_analyze) == 0:\n",
    "        print(\"   All reviews already have sentiment scores!\")\n",
    "    else:\n",
    "        analyzed_count = 0\n",
    "        \n",
    "        for i, review in enumerate(reviews_to_analyze, 1):\n",
    "            if i % 100 == 0:\n",
    "                print(f\"   Progress: {i}/{len(reviews_to_analyze)}\")\n",
    "            \n",
    "            try:\n",
    "                # Run VADER sentiment analysis\n",
    "                scores = analyzer.polarity_scores(review.text)\n",
    "                \n",
    "                # Compound score ranges from -1 (most negative) to +1 (most positive)\n",
    "                compound_score = scores['compound']\n",
    "                \n",
    "                # Determine sentiment label\n",
    "                if compound_score >= 0.05:\n",
    "                    sentiment_label = 'positive'\n",
    "                elif compound_score <= -0.05:\n",
    "                    sentiment_label = 'negative'\n",
    "                else:\n",
    "                    sentiment_label = 'neutral'\n",
    "                \n",
    "                # Update review with sentiment\n",
    "                review.sentiment_score = round(compound_score, 6)  # 6 decimal places\n",
    "                review.sentiment_label = sentiment_label\n",
    "                review.sentiment_confidence = round(max(scores['pos'], scores['neg'], scores['neu']), 4)\n",
    "                \n",
    "                analyzed_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"   Error analyzing review {review.id}: {str(e)[:50}\")\")\n",
    "                continue\n",
    "        \n",
    "        # Commit all updates\n",
    "        db.commit()\n",
    "        \n",
    "        print(f\"\\nSentiment analysis complete!\")\n",
    "        print(f\"   Analyzed: {analyzed_count} reviews\")\n",
    "    \n",
    "    # Show distribution\n",
    "    from sqlalchemy import func\n",
    "    sentiment_distribution = db.query(\n",
    "        Review.sentiment_label, \n",
    "        func.count(Review.id)\n",
    "    ).group_by(Review.sentiment_label).all()\n",
    "    \n",
    "    print(f\"\\nSentiment Distribution:\")\n",
    "    for label, count in sentiment_distribution:\n",
    "        if label:  # Skip None values\n",
    "            print(f\"   {label.capitalize()}: {count}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during sentiment analysis: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    db.rollback()\n",
    "finally:\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9e650",
   "metadata": {},
   "source": [
    "## Step 15: Calculate Average Sentiment Per Movie\n",
    "\n",
    "Calculate average sentiment for each movie by source (IMDb, RT categories) and update the movies table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf80a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\" CALCULATING AVERAGE SENTIMENT PER MOVIE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from sqlalchemy import func\n",
    "\n",
    "# Reconnect to database\n",
    "db = SessionLocal()\n",
    "\n",
    "try:\n",
    "    # Get all movies that have reviews\n",
    "    movies_with_reviews = db.query(Movie).join(Review).distinct().all()\n",
    "    \n",
    "    print(f\"\\nüìΩÔ∏è  Processing {len(movies_with_reviews)} movies with reviews\")\n",
    "    \n",
    "    updated_count = 0\n",
    "    \n",
    "    for movie in movies_with_reviews:\n",
    "        print(f\"\\n{movie.title} ({movie.release_year}):\")\n",
    "        \n",
    "        # IMDb sentiment average\n",
    "        imdb_sentiment = db.query(func.avg(Review.sentiment_score)).filter(\n",
    "            Review.movie_id == movie.id,\n",
    "            Review.source == 'imdb',\n",
    "            Review.sentiment_score.isnot(None)\n",
    "        ).scalar()\n",
    "        \n",
    "        if imdb_sentiment:\n",
    "            movie.sentiment_imdb_avg = round(float(imdb_sentiment), 6)\n",
    "            \n",
    "            count = db.query(func.count(Review.id)).filter(\n",
    "                Review.movie_id == movie.id,\n",
    "                Review.source == 'imdb',\n",
    "                Review.sentiment_score.isnot(None)\n",
    "            ).scalar()\n",
    "            print(f\"   IMDb avg sentiment: {movie.sentiment_imdb_avg:.4f} (n={count})\")\n",
    "        \n",
    "        # Rotten Tomatoes - separate by review_category\n",
    "        # Categories: 'top_critic', 'critic', 'verified_audience', 'audience'\n",
    "        \n",
    "        # RT Top Critics\n",
    "        rt_top_critics_sentiment = db.query(func.avg(Review.sentiment_score)).filter(\n",
    "            Review.movie_id == movie.id,\n",
    "            Review.source == 'rotten_tomatoes',\n",
    "            Review.review_category == 'top_critic',\n",
    "            Review.sentiment_score.isnot(None)\n",
    "        ).scalar()\n",
    "        \n",
    "        if rt_top_critics_sentiment:\n",
    "            movie.sentiment_rt_top_critics_avg = round(float(rt_top_critics_sentiment), 6)\n",
    "            \n",
    "            count = db.query(func.count(Review.id)).filter(\n",
    "                Review.movie_id == movie.id,\n",
    "                Review.source == 'rotten_tomatoes',\n",
    "                Review.review_category == 'top_critic',\n",
    "                Review.sentiment_score.isnot(None)\n",
    "            ).scalar()\n",
    "            print(f\"   RT Top Critics avg sentiment: {movie.sentiment_rt_top_critics_avg:.4f} (n={count})\")\n",
    "        \n",
    "        # RT All Critics\n",
    "        rt_all_critics_sentiment = db.query(func.avg(Review.sentiment_score)).filter(\n",
    "            Review.movie_id == movie.id,\n",
    "            Review.source == 'rotten_tomatoes',\n",
    "            Review.review_category == 'critic',\n",
    "            Review.sentiment_score.isnot(None)\n",
    "        ).scalar()\n",
    "        \n",
    "        if rt_all_critics_sentiment:\n",
    "            movie.sentiment_rt_all_critics_avg = round(float(rt_all_critics_sentiment), 6)\n",
    "            \n",
    "            count = db.query(func.count(Review.id)).filter(\n",
    "                Review.movie_id == movie.id,\n",
    "                Review.source == 'rotten_tomatoes',\n",
    "                Review.review_category == 'critic',\n",
    "                Review.sentiment_score.isnot(None)\n",
    "            ).scalar()\n",
    "            print(f\"   RT All Critics avg sentiment: {movie.sentiment_rt_all_critics_avg:.4f} (n={count})\")\n",
    "        \n",
    "        # RT Verified Audience\n",
    "        rt_verified_sentiment = db.query(func.avg(Review.sentiment_score)).filter(\n",
    "            Review.movie_id == movie.id,\n",
    "            Review.source == 'rotten_tomatoes',\n",
    "            Review.review_category == 'verified_audience',\n",
    "            Review.sentiment_score.isnot(None)\n",
    "        ).scalar()\n",
    "        \n",
    "        if rt_verified_sentiment:\n",
    "            movie.sentiment_rt_verified_audience_avg = round(float(rt_verified_sentiment), 6)\n",
    "            \n",
    "            count = db.query(func.count(Review.id)).filter(\n",
    "                Review.movie_id == movie.id,\n",
    "                Review.source == 'rotten_tomatoes',\n",
    "                Review.review_category == 'verified_audience',\n",
    "                Review.sentiment_score.isnot(None)\n",
    "            ).scalar()\n",
    "            print(f\"   RT Verified Audience avg sentiment: {movie.sentiment_rt_verified_audience_avg:.4f} (n={count})\")\n",
    "        \n",
    "        # RT All Audience\n",
    "        rt_audience_sentiment = db.query(func.avg(Review.sentiment_score)).filter(\n",
    "            Review.movie_id == movie.id,\n",
    "            Review.source == 'rotten_tomatoes',\n",
    "            Review.review_category == 'audience',\n",
    "            Review.sentiment_score.isnot(None)\n",
    "        ).scalar()\n",
    "        \n",
    "        if rt_audience_sentiment:\n",
    "            movie.sentiment_rt_all_audience_avg = round(float(rt_audience_sentiment), 6)\n",
    "            \n",
    "            count = db.query(func.count(Review.id)).filter(\n",
    "                Review.movie_id == movie.id,\n",
    "                Review.source == 'rotten_tomatoes',\n",
    "                Review.review_category == 'audience',\n",
    "                Review.sentiment_score.isnot(None)\n",
    "            ).scalar()\n",
    "            print(f\"   RT All Audience avg sentiment: {movie.sentiment_rt_all_audience_avg:.4f} (n={count})\")\n",
    "        \n",
    "        updated_count += 1\n",
    "    \n",
    "    # Commit all updates\n",
    "    db.commit()\n",
    "    \n",
    "    print(f\"\\n Sentiment averages calculated for {updated_count} movies\")\n",
    "    \n",
    "    # Show summary statistics\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\" SENTIMENT SUMMARY ACROSS ALL MOVIES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Overall averages\n",
    "    overall_imdb = db.query(func.avg(Movie.sentiment_imdb_avg)).filter(\n",
    "        Movie.sentiment_imdb_avg.isnot(None)\n",
    "    ).scalar()\n",
    "    \n",
    "    overall_rt_top = db.query(func.avg(Movie.sentiment_rt_top_critics_avg)).filter(\n",
    "        Movie.sentiment_rt_top_critics_avg.isnot(None)\n",
    "    ).scalar()\n",
    "    \n",
    "    overall_rt_all = db.query(func.avg(Movie.sentiment_rt_all_critics_avg)).filter(\n",
    "        Movie.sentiment_rt_all_critics_avg.isnot(None)\n",
    "    ).scalar()\n",
    "    \n",
    "    overall_rt_verified = db.query(func.avg(Movie.sentiment_rt_verified_audience_avg)).filter(\n",
    "        Movie.sentiment_rt_verified_audience_avg.isnot(None)\n",
    "    ).scalar()\n",
    "    \n",
    "    overall_rt_audience = db.query(func.avg(Movie.sentiment_rt_all_audience_avg)).filter(\n",
    "        Movie.sentiment_rt_all_audience_avg.isnot(None)\n",
    "    ).scalar()\n",
    "    \n",
    "    if overall_imdb:\n",
    "        print(f\"IMDb Average Sentiment: {float(overall_imdb):.4f}\")\n",
    "    if overall_rt_top:\n",
    "        print(f\"RT Top Critics Average Sentiment: {float(overall_rt_top):.4f}\")\n",
    "    if overall_rt_all:\n",
    "        print(f\"RT All Critics Average Sentiment: {float(overall_rt_all):.4f}\")\n",
    "    if overall_rt_verified:\n",
    "        print(f\"RT Verified Audience Average Sentiment: {float(overall_rt_verified):.4f}\")\n",
    "    if overall_rt_audience:\n",
    "        print(f\"RT All Audience Average Sentiment: {float(overall_rt_audience):.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\" Error calculating sentiment averages: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    db.rollback()\n",
    "finally:\n",
    "    db.close()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" ALL SENTIMENT PROCESSING COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958971e",
   "metadata": {},
   "source": [
    "## Step 16: Generate Search Terms for Reddit (Optional - Future Feature)\n",
    "\n",
    "** This step is for future Reddit scraping.**\n",
    "\n",
    "Use OpenAI GPT to generate smart search terms for finding movie discussions on Reddit.\n",
    "This is where your $10 OpenAI credits will be useful!\n",
    "\n",
    "**To enable later:**\n",
    "1. Get Reddit API credentials\n",
    "2. Uncomment and run the cell below\n",
    "3. Use search terms with the Reddit scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUTURE FEATURE - Uncomment when ready to use Reddit API\n",
    "# from scrapers.openai_search import OpenAISearchTermGenerator\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# \n",
    "# print(f\" Generating Reddit search terms for {TEST_MOVIES} movies...\\n\")\n",
    "# \n",
    "# # Initialize OpenAI generator\n",
    "# openai_generator = OpenAISearchTermGenerator()\n",
    "# \n",
    "# def generate_reddit_terms(movie):\n",
    "#     \"\"\"Generate Reddit-specific search terms\"\"\"\n",
    "#     try:\n",
    "#         print(f\"üìΩÔ∏è  {movie.title} ({movie.release_year})\")\n",
    "#         \n",
    "#         search_terms_dict = openai_generator.generate_search_terms(\n",
    "#             title=movie.title,\n",
    "#             year=movie.release_year,\n",
    "#             genres=movie.genres.split('|') if movie.genres else [],\n",
    "#             overview=movie.overview or \"\"\n",
    "#         )\n",
    "#         \n",
    "#         if search_terms_dict and 'reddit' in search_terms_dict:\n",
    "#             reddit_terms = search_terms_dict['reddit']\n",
    "#             print(f\"    Generated {len(reddit_terms)} Reddit search terms\")\n",
    "#             return (movie, reddit_terms, True)\n",
    "#         return (movie, [], False)\n",
    "#     except Exception as e:\n",
    "#         print(f\"    Error: {str(e)[:100]}\")\n",
    "#         return (movie, [], False)\n",
    "# \n",
    "# # Run concurrent generation\n",
    "# reddit_results = []\n",
    "# with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#     futures = {executor.submit(generate_reddit_terms, movie): movie \n",
    "#                for movie in movies_to_scrape}\n",
    "#     for future in as_completed(futures):\n",
    "#         reddit_results.append(future.result())\n",
    "# \n",
    "# # Save to database\n",
    "# db = SessionLocal()\n",
    "# try:\n",
    "#     successful = 0\n",
    "#     for movie, terms, success in reddit_results:\n",
    "#         if success and terms:\n",
    "#             for term in terms[:5]:  # Limit to 5 terms per movie\n",
    "#                 search_term_obj = MovieSearchTerm(\n",
    "#                     movie_id=movie.id,\n",
    "#                     search_term=term,\n",
    "#                     source='openai_reddit',\n",
    "#                     created_at=datetime.now()\n",
    "#                 )\n",
    "#                 db.add(search_term_obj)\n",
    "#             successful += 1\n",
    "#     db.commit()\n",
    "#     print(f\"\\n Saved Reddit search terms for {successful}/{TEST_MOVIES} movies\")\n",
    "# except Exception as e:\n",
    "#     print(f\" Error: {e}\")\n",
    "#     db.rollback()\n",
    "# finally:\n",
    "#     db.close()\n",
    "\n",
    "print(\" Reddit search term generation - Ready for future use!\")\n",
    "print(\"   Uncomment the code above when you have Reddit API credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c2440",
   "metadata": {},
   "source": [
    "## Step 17: Analyze All Collected Reviews\n",
    "\n",
    "View comprehensive statistics and samples from IMDb and Rotten Tomatoes reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835052c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all reviews from database\n",
    "db = SessionLocal()\n",
    "all_reviews = db.query(Review).all()\n",
    "\n",
    "if all_reviews:\n",
    "    print(\" REVIEW COLLECTION SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Overall statistics\n",
    "    imdb_reviews = [r for r in all_reviews if r.source == 'imdb']\n",
    "    rt_reviews = [r for r in all_reviews if r.source == 'rotten_tomatoes']\n",
    "    rt_critic = [r for r in rt_reviews if 'critic' in (r.source_id or '')]\n",
    "    rt_audience = [r for r in rt_reviews if 'audience' in (r.source_id or '')]\n",
    "    \n",
    "    print(f\"\\n Overall Statistics:\")\n",
    "    print(f\"   Total reviews collected: {len(all_reviews)}\")\n",
    "    print(f\"   IMDb reviews: {len(imdb_reviews)}\")\n",
    "    print(f\"   Rotten Tomatoes reviews: {len(rt_reviews)}\")\n",
    "    print(f\"      ‚Ä¢ Critic reviews: {len(rt_critic)}\")\n",
    "    print(f\"      ‚Ä¢ Audience reviews: {len(rt_audience)}\")\n",
    "    \n",
    "    # Average lengths\n",
    "    if all_reviews:\n",
    "        avg_length_all = sum(r.review_length or 0 for r in all_reviews) / len(all_reviews)\n",
    "        avg_words_all = sum(r.word_count or 0 for r in all_reviews) / len(all_reviews)\n",
    "        print(f\"\\n Review Metrics:\")\n",
    "        print(f\"   Average review length: {avg_length_all:.0f} characters\")\n",
    "        print(f\"   Average word count: {avg_words_all:.0f} words\")\n",
    "    \n",
    "    if imdb_reviews:\n",
    "        avg_imdb_length = sum(r.review_length or 0 for r in imdb_reviews) / len(imdb_reviews)\n",
    "        avg_imdb_rating = sum(r.rating or 0 for r in imdb_reviews if r.rating) / len([r for r in imdb_reviews if r.rating])\n",
    "        print(f\"\\n IMDb Reviews:\")\n",
    "        print(f\"   Average length: {avg_imdb_length:.0f} characters\")\n",
    "        print(f\"   Average rating: {avg_imdb_rating:.2f}/10\")\n",
    "        print(f\"   Total helpful votes: {sum(r.helpful_count or 0 for r in imdb_reviews):,}\")\n",
    "    \n",
    "    # Per movie breakdown\n",
    "    print(f\"\\n\\n REVIEWS BY MOVIE:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    movies_with_reviews = db.query(Movie).join(Review).distinct().all()\n",
    "    \n",
    "    for movie in movies_with_reviews:\n",
    "        movie_reviews = [r for r in all_reviews if r.movie_id == movie.id]\n",
    "        imdb_count = len([r for r in movie_reviews if r.source == 'imdb'])\n",
    "        rt_count = len([r for r in movie_reviews if r.source == 'rotten_tomatoes'])\n",
    "        \n",
    "        print(f\"\\nüìΩÔ∏è  {movie.title} ({movie.release_year})\")\n",
    "        print(f\"   Total reviews: {len(movie_reviews)}\")\n",
    "        print(f\"   IMDb: {imdb_count} | Rotten Tomatoes: {rt_count}\")\n",
    "        \n",
    "        # Show sample review from each source\n",
    "        sample_imdb = next((r for r in movie_reviews if r.source == 'imdb'), None)\n",
    "        if sample_imdb:\n",
    "            text_preview = sample_imdb.text[:120] + \"...\" if len(sample_imdb.text) > 120 else sample_imdb.text\n",
    "            rating_str = f\"{sample_imdb.rating}/10\" if sample_imdb.rating else \"No rating\"\n",
    "            print(f\"\\n    Sample IMDb Review ({rating_str}):\")\n",
    "            print(f\"      \\\"{text_preview}\\\"\")\n",
    "            print(f\"      Author: {sample_imdb.author or 'Anonymous'} | {sample_imdb.helpful_count or 0} helpful votes\")\n",
    "        \n",
    "        sample_rt = next((r for r in movie_reviews if r.source == 'rotten_tomatoes'), None)\n",
    "        if sample_rt:\n",
    "            text_preview = sample_rt.text[:120] + \"...\" if len(sample_rt.text) > 120 else sample_rt.text\n",
    "            review_type = 'Critic' if 'critic' in (sample_rt.source_id or '') else 'Audience'\n",
    "            print(f\"\\n    Sample RT Review ({review_type}):\")\n",
    "            print(f\"      \\\"{text_preview}\\\"\")\n",
    "            print(f\"      Author: {sample_rt.author or 'Anonymous'}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\" No reviews collected yet\")\n",
    "    print(\"   Run Steps 11a and 11b to scrape reviews\")\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a817443",
   "metadata": {},
   "source": [
    "## Step 18: Visualize Review Data\n",
    "\n",
    "Create visualizations to understand the review collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef857f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SessionLocal()\n",
    "all_reviews = db.query(Review).all()\n",
    "db.close()\n",
    "\n",
    "if all_reviews and len(all_reviews) > 0:\n",
    "    # Prepare data for visualization\n",
    "    review_data = []\n",
    "    for review in all_reviews:\n",
    "        review_data.append({\n",
    "            'source': review.source,\n",
    "            'length': review.review_length or 0,\n",
    "            'word_count': review.word_count or 0,\n",
    "            'rating': review.rating,\n",
    "            'helpful_count': review.helpful_count or 0\n",
    "        })\n",
    "    \n",
    "    df_reviews = pd.DataFrame(review_data)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Reviews by Source\n",
    "    ax1 = axes[0, 0]\n",
    "    source_counts = df_reviews['source'].value_counts()\n",
    "    colors = {'imdb': '#f5c518', 'rotten_tomatoes': '#fa320a'}\n",
    "    ax1.bar(source_counts.index, source_counts.values, \n",
    "            color=[colors.get(s, 'gray') for s in source_counts.index])\n",
    "    ax1.set_title('Reviews by Source', fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel('Number of Reviews')\n",
    "    ax1.set_xlabel('Source')\n",
    "    \n",
    "    # 2. Review Length Distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    imdb_lengths = df_reviews[df_reviews['source'] == 'imdb']['length']\n",
    "    rt_lengths = df_reviews[df_reviews['source'] == 'rotten_tomatoes']['length']\n",
    "    \n",
    "    if len(imdb_lengths) > 0:\n",
    "        ax2.hist(imdb_lengths, bins=20, alpha=0.6, label='IMDb', color='#f5c518', edgecolor='black')\n",
    "    if len(rt_lengths) > 0:\n",
    "        ax2.hist(rt_lengths, bins=20, alpha=0.6, label='Rotten Tomatoes', color='#fa320a', edgecolor='black')\n",
    "    \n",
    "    ax2.set_title('Review Length Distribution', fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel('Review Length (characters)')\n",
    "    ax2.set_ylabel('Number of Reviews')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Rating Distribution (IMDb only, as RT uses binary fresh/rotten)\n",
    "    ax3 = axes[1, 0]\n",
    "    imdb_ratings = df_reviews[(df_reviews['source'] == 'imdb') & (df_reviews['rating'].notna())]['rating']\n",
    "    \n",
    "    if len(imdb_ratings) > 0:\n",
    "        ax3.hist(imdb_ratings, bins=10, color='#f5c518', alpha=0.7, edgecolor='black')\n",
    "        ax3.axvline(imdb_ratings.mean(), color='red', linestyle='--', \n",
    "                   label=f'Mean: {imdb_ratings.mean():.2f}')\n",
    "        ax3.set_title('IMDb Review Ratings Distribution', fontsize=14, fontweight='bold')\n",
    "        ax3.set_xlabel('Rating (0-10)')\n",
    "        ax3.set_ylabel('Number of Reviews')\n",
    "        ax3.legend()\n",
    "    else:\n",
    "        ax3.text(0.5, 0.5, 'No IMDb ratings available', \n",
    "                ha='center', va='center', transform=ax3.transAxes)\n",
    "        ax3.set_title('IMDb Review Ratings Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 4. Word Count by Source\n",
    "    ax4 = axes[1, 1]\n",
    "    source_word_counts = df_reviews.groupby('source')['word_count'].mean()\n",
    "    ax4.bar(source_word_counts.index, source_word_counts.values,\n",
    "            color=[colors.get(s, 'gray') for s in source_word_counts.index])\n",
    "    ax4.set_title('Average Words per Review', fontsize=14, fontweight='bold')\n",
    "    ax4.set_ylabel('Average Word Count')\n",
    "    ax4.set_xlabel('Source')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(source_word_counts.values):\n",
    "        ax4.text(i, v + 5, f'{v:.0f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Review visualizations complete!\")\n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"   Total reviews: {len(df_reviews)}\")\n",
    "    print(f\"   Average review length: {df_reviews['length'].mean():.0f} characters\")\n",
    "    print(f\"   Average word count: {df_reviews['word_count'].mean():.0f} words\")\n",
    "    \n",
    "else:\n",
    "    print(\"No reviews to visualize yet\")\n",
    "    print(\"   Run Steps 11a and 12 to scrape reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35d1b62",
   "metadata": {},
   "source": [
    "## Step 19: Compare Ratings & Analyze Review Collection\n",
    "\n",
    "See how ratings differ between sources and explore the comprehensive review data collected from IMDb and Rotten Tomatoes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1274c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch updated movies\n",
    "db = SessionLocal()\n",
    "scraped_movies = db.query(Movie).filter(Movie.imdb_rating.isnot(None)).limit(TEST_MOVIES).all()\n",
    "db.close()\n",
    "\n",
    "if scraped_movies:\n",
    "    print(\" RATING COMPARISON (TMDB vs IMDb)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    comparison_data = []\n",
    "    for movie in scraped_movies:\n",
    "        rating_info = movie.get_rating_metadata()\n",
    "        \n",
    "        print(f\"\\n {movie.title} ({movie.release_year})\")\n",
    "        print(f\"   TMDB Rating:  {movie.tmdb_rating or 'N/A'}/10 ({movie.tmdb_vote_count or 0:,} votes)\")\n",
    "        print(f\"   IMDb Rating:  {movie.imdb_rating or 'N/A'}/10 ({movie.imdb_vote_count or 0:,} votes)\")\n",
    "        print(f\"   Best Rating:  {rating_info['recommended_rating']}/10\")\n",
    "        \n",
    "        if movie.tmdb_rating and movie.imdb_rating:\n",
    "            diff = abs(movie.tmdb_rating - movie.imdb_rating)\n",
    "            print(f\"   Difference:   {diff:.2f} points\")\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'title': movie.title,\n",
    "            'tmdb': movie.tmdb_rating,\n",
    "            'imdb': movie.imdb_rating,\n",
    "            'best': rating_info['recommended_rating']\n",
    "        })\n",
    "    \n",
    "    # Visualization\n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    x = range(len(df_comparison))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax.bar([i - width/2 for i in x], df_comparison['tmdb'], width, label='TMDB', alpha=0.8, color='#01b4e4')\n",
    "    ax.bar([i + width/2 for i in x], df_comparison['imdb'], width, label='IMDb', alpha=0.8, color='#f5c518')\n",
    "    \n",
    "    ax.set_xlabel('Movies')\n",
    "    ax.set_ylabel('Rating (0-10)')\n",
    "    ax.set_title('TMDB vs IMDb Ratings Comparison', fontweight='bold', fontsize=14)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([t[:20] for t in df_comparison['title']], rotation=45, ha='right')\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No scraped movies found with IMDb ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e1d785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comprehensive review data summary\n",
    "db = SessionLocal()\n",
    "\n",
    "# Get all reviews\n",
    "all_reviews = db.query(Review).all()\n",
    "\n",
    "if all_reviews:\n",
    "    print(\"\udcca COMPREHENSIVE REVIEW DATA SUMMARY\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Overall stats by source\n",
    "    imdb_reviews = [r for r in all_reviews if r.source == 'imdb']\n",
    "    rt_reviews = [r for r in all_reviews if r.source == 'rotten_tomatoes']\n",
    "    \n",
    "    print(f\"\\n\udcc8 Overall Statistics:\")\n",
    "    print(f\"   Total reviews: {len(all_reviews)}\")\n",
    "    print(f\"   IMDb reviews: {len(imdb_reviews)}\")\n",
    "    print(f\"   Rotten Tomatoes reviews: {len(rt_reviews)}\")\n",
    "    \n",
    "    if imdb_reviews:\n",
    "        total_helpful = sum(r.helpful_count or 0 for r in imdb_reviews)\n",
    "        avg_length = sum(r.review_length or 0 for r in imdb_reviews) / len(imdb_reviews)\n",
    "        avg_rating = sum(r.rating or 0 for r in imdb_reviews if r.rating) / max(len([r for r in imdb_reviews if r.rating]), 1)\n",
    "        \n",
    "        print(f\"\\n IMDb Review Metrics:\")\n",
    "        print(f\"   Total helpful votes: {total_helpful:,}\")\n",
    "        print(f\"   Average review length: {avg_length:.0f} characters\")\n",
    "        print(f\"   Average rating: {avg_rating:.2f}/10\")\n",
    "        print(f\"   Average words per review: {sum(r.word_count or 0 for r in imdb_reviews) / len(imdb_reviews):.0f}\")\n",
    "    \n",
    "    if rt_reviews:\n",
    "        avg_length_rt = sum(r.review_length or 0 for r in rt_reviews) / len(rt_reviews)\n",
    "        critic_reviews = [r for r in rt_reviews if 'critic' in (r.source_id or '')]\n",
    "        audience_reviews = [r for r in rt_reviews if 'audience' in (r.source_id or '')]\n",
    "        \n",
    "        print(f\"\\n Rotten Tomatoes Metrics:\")\n",
    "        print(f\"   Critic reviews: {len(critic_reviews)}\")\n",
    "        print(f\"   Audience reviews: {len(audience_reviews)}\")\n",
    "        print(f\"   Average review length: {avg_length_rt:.0f} characters\")\n",
    "        print(f\"   Average words per review: {sum(r.word_count or 0 for r in rt_reviews) / len(rt_reviews):.0f}\")\n",
    "    \n",
    "    # Per movie breakdown\n",
    "    print(f\"\\n\\n Reviews Per Movie:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    movies_with_reviews = db.query(Movie).join(Review).distinct().all()\n",
    "    \n",
    "    for movie in movies_with_reviews:\n",
    "        movie_reviews = [r for r in all_reviews if r.movie_id == movie.id]\n",
    "        imdb_count = len([r for r in movie_reviews if r.source == 'imdb'])\n",
    "        rt_count = len([r for r in movie_reviews if r.source == 'rotten_tomatoes'])\n",
    "        \n",
    "        if imdb_count > 0 or rt_count > 0:\n",
    "            print(f\"\\nüìΩÔ∏è  {movie.title} ({movie.release_year})\")\n",
    "            print(f\"   Total reviews: {len(movie_reviews)}\")\n",
    "            print(f\"   IMDb: {imdb_count} | Rotten Tomatoes: {rt_count}\")\n",
    "            \n",
    "            # Sample review\n",
    "            sample = movie_reviews[0] if movie_reviews else None\n",
    "            if sample:\n",
    "                text_preview = sample.text[:100] + \"...\" if len(sample.text) > 100 else sample.text\n",
    "                source_name = \"IMDb\" if sample.source == 'imdb' else \"Rotten Tomatoes\"\n",
    "                print(f\"    Sample ({source_name}): \\\"{text_preview}\\\"\")\n",
    "    \n",
    "    # Create visualization: Reviews per movie\n",
    "    if len(movies_with_reviews) > 0:\n",
    "        print(f\"\\n Creating visualization...\")\n",
    "        \n",
    "        movie_review_counts = {}\n",
    "        movie_sources = {}\n",
    "        \n",
    "        for movie in movies_with_reviews:\n",
    "            reviews = [r for r in all_reviews if r.movie_id == movie.id]\n",
    "            if reviews:\n",
    "                movie_name = movie.title[:20]\n",
    "                movie_review_counts[movie_name] = len(reviews)\n",
    "                imdb_cnt = len([r for r in reviews if r.source == 'imdb'])\n",
    "                rt_cnt = len([r for r in reviews if r.source == 'rotten_tomatoes'])\n",
    "                movie_sources[movie_name] = {'imdb': imdb_cnt, 'rt': rt_cnt}\n",
    "        \n",
    "        if movie_review_counts:\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            # Reviews per movie - stacked bar\n",
    "            movies = list(movie_review_counts.keys())\n",
    "            imdb_counts = [movie_sources[m]['imdb'] for m in movies]\n",
    "            rt_counts = [movie_sources[m]['rt'] for m in movies]\n",
    "            \n",
    "            ax1.barh(movies, imdb_counts, label='IMDb', color='#f5c518')\n",
    "            ax1.barh(movies, rt_counts, left=imdb_counts, label='Rotten Tomatoes', color='#fa320a')\n",
    "            ax1.set_xlabel('Number of Reviews')\n",
    "            ax1.set_title('Reviews per Movie (by Source)', fontweight='bold', fontsize=14)\n",
    "            ax1.legend()\n",
    "            ax1.invert_yaxis()\n",
    "            \n",
    "            # Review length distribution\n",
    "            review_lengths = [r.review_length for r in all_reviews if r.review_length]\n",
    "            if review_lengths:\n",
    "                ax2.hist(review_lengths, bins=30, color='#4CAF50', alpha=0.7, edgecolor='black')\n",
    "                ax2.set_xlabel('Review Length (characters)')\n",
    "                ax2.set_ylabel('Number of Reviews')\n",
    "                ax2.set_title('Review Length Distribution', fontweight='bold', fontsize=14)\n",
    "                ax2.axvline(sum(review_lengths)/len(review_lengths), color='red', linestyle='--', \n",
    "                           label=f'Mean: {sum(review_lengths)/len(review_lengths):.0f} chars')\n",
    "                ax2.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    \n",
    "else:\n",
    "    print(\"\udcca No review data collected yet\")\n",
    "    print(\"   Run Steps 11a and 11b to scrape IMDb and Rotten Tomatoes reviews\")\n",
    "\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47add24d",
   "metadata": {},
   "source": [
    "## Quick Test with Full Scraping Complete!\n",
    "\n",
    "**What we tested:**\n",
    "- CSV loading and parsing (random selection)\n",
    "- Database operations with auto-clearing\n",
    "- Random movie selection for scraping\n",
    "- IMDb rating scraping (concurrent - up to 3 at once)\n",
    "- IMDb review scraping (concurrent - up to 3 at once, 30 reviews per movie)\n",
    "- Rotten Tomatoes review scraping (concurrent - up to 3 at once, 20 reviews per movie)\n",
    "- Rating comparison logic (TMDB vs IMDb)\n",
    "- Smart rating selection\n",
    "- Multi-source review analysis and visualization\n",
    "\n",
    "**Performance:**\n",
    "- Concurrent scraping speeds up the process significantly\n",
    "- Random movie selection ensures variety in testing\n",
    "- ThreadPoolExecutor manages parallel requests efficiently\n",
    "- No OpenAI API calls needed for IMDb/RT scraping (saves credits!)\n",
    "\n",
    "**Data Sources:**\n",
    "- TMDB: Ratings from CSV dataset\n",
    "- IMDb: Live scraped ratings + detailed user reviews\n",
    "- Rotten Tomatoes: Critic and audience reviews\n",
    "- OpenAI: Reserved for Reddit search term generation (future feature)\n",
    "\n",
    "**Review Collection:**\n",
    "- IMDb reviews include: title, text, rating (1-10), author, date, helpful votes\n",
    "- RT reviews include: critic/audience distinction, text, author, date\n",
    "- Comprehensive review analytics and visualizations\n",
    "\n",
    "**OpenAI Usage:**\n",
    "- OpenAI API credits saved for Reddit scraping when you get API access\n",
    "- IMDb and RT scrapers work directly without needing OpenAI\n",
    "- Efficient use of API credits only where truly needed\n",
    "\n",
    "**Next Steps:**\n",
    "1. Run full pipeline on more movies\n",
    "2. Scale to full dataset (2000 movies) with concurrent processing\n",
    "3. Get Reddit API credentials and enable Step 16 for Reddit discussions\n",
    "4. Implement sentiment analysis on review data\n",
    "5. Build recommendation models using hybrid data sources (TMDB + IMDb + RT reviews + Reddit)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
